---
title: "Assignment 3"
author: "Ajay Arora, Romerl Elizes, Jimmy Ng, Joshua Registe, Adam Rich"
date: "3/21/21"
output:
  rmdformats::readthedown:
    self_contained: yes
    thumbnails: yes
    lightbox: yes
    gallery: no
    highlight: tango
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

if(!require(wrapr)){install.packages(wrapr); require(wrapr)}

packages <- wrapr::qc(palmerpenguins, htmlTable, gmodels, class, dplyr, corrplot, car, caret, corrplot, tidyverse, kableExtra, mice, randomForest, readxl, tidyselect, conflicted, naniar, skimr, DataExplorer, tidymodels, broom, dotwhisker, vip, rmarkdown, rsample, parsnip, workflows, recipes, tune, yardstick)

pacman::p_load(char = packages)

defaulttheme <- theme(panel.background = element_blank(),
                      panel.border = element_rect(color = "black", fill = NA))
```

## (Q1a) KNN - EDA {.tabset .tabset-fade .tabset-pills}

Please use K-nearestneighbor (KNN) algorithm to predict the species variable.Please be sure to walk through the steps you took. (40points)

What is **kNN Algorithm?**

Let’s assume we have several groups of labeled samples. The items present in the groups are homogeneous in nature. Now, suppose we have an unlabeled example which needs to be classified into one of the several labeled groups. How do you do that? Unhesitatingly, using kNN Algorithm.

k nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. This algorithms segregates unlabeled data points into well defined groups.

### Data Type & Format

A tibble with 344 rows and 8 variables:
species

    a factor denoting penguin species (Adélie, Chinstrap and Gentoo)
island

    a factor denoting island in Palmer Archipelago, Antarctica (Biscoe, Dream or Torgersen)
bill_length_mm

    a number denoting bill length (millimeters)
bill_depth_mm

    a number denoting bill depth (millimeters)
flipper_length_mm

    an integer denoting flipper length (millimeters)
body_mass_g

    an integer denoting body mass (grams)
sex

    a factor denoting penguin sex (female, male)
year

    an integer denoting the study year (2007, 2008, or 2009)
    
### Summary

The summary table provides some simple statistics of the penguin data.  E.g., we see a total number per species (Adelie, Chinstrap & Gentoo) as well as the number on each different island.  In addition, we that each island has a certain number of penguins present. Furthermore, we that each of the quatifiable variables (bill_length_mm, bill_depth_mm, flipper_length_mm, & body_mass_g) shows distribution of the numbers. We have a minimum, maximum, average, median, etc.  In addition, we also see a number of missing values.  Lastly, we  see number of species by sex distribution and the range of years in the which the data was obtained.

```{r}
summary(penguins)
```

### Missing data

Mitigate any missing data

We have opted to remove missing data observations versus impute the missing values.  For numerical data, the mean would not change if we imputed, but, variance decreases and changes.(1) The 4 variables (bill_length_mm  bill_depth_mm   flipper_length_mm  body_mass_g) all have the same number (2) of missing data.  By removing these observations, we are treating all of these variables in the same manner (equally), which does not introduce bias.  Lastly, variable sex has 11 missing data points.  This was removed because the number of observations between female and male are roughly the same number.  If we attempted to artifically account for the missing data points, we could be adding bias.  Given, it is a small dataset, most of the data points are intact and we don't believe that the model will lose power during modeling.  If we have added values artifically, we would add bias because it is a small dataset and this would lead to a magnified variance.  We opted to keep the data integrity pure as possible. (6)

```{r}
penguins_df = na.omit(penguins)
summary(penguins_df)
```

### Correlation 

Correlation between numerical variables

**Note**: We are using the following definitions for variable correlation. For this test the correlation threshold is set at 0.5. 

Correlation coefficients whose magnitude are between 0.9 and 1.0 indicate variables which can be considered very highly correlated. Correlation coefficients whose magnitude are between 0.7 and 0.9 indicate variables which can be considered highly correlated. Correlation coefficients whose magnitude are between 0.5 and 0.7 indicate variables which can be considered moderately correlated. Correlation coefficients whose magnitude are between 0.3 and 0.5 indicate variables which have a low correlation. Correlation coefficients whose magnitude are less than 0.3 have little if any (linear) correlation. (2)

```{r fig.height = 9, fig.width = 12, fig.align = "center"}
# Select numericals
penNumeric <- penguins_df %>% select(-species, -sex, -island)
# Cor-relation between numerics
corrplot(cor(penNumeric), type = 'lower', diag = FALSE)

result1 <- cor(subset(penguins_df, select = c("bill_length_mm", "bill_depth_mm", "body_mass_g", "year")))
result1


corr_check_with_variables <- function(data, t){
  m <- cor(data)
  

  for (i in 1:nrow(m)){
    c <-  which((abs(m[i,i:ncol(m)]) > t) & (m[i,i:ncol(m)] != 1))
  
    if(length(c)> 0){
      lapply(c,FUN =  function(x) (cat(paste(colnames(data)[i], "with",colnames(data)[x]), "\n")))
     
    }
  }
}
#(3)

corr_check_with_variables(penNumeric, 0.5)

```

In this step, we decided to remove Year predictor from classification.  This was done because of the low contribution of Year with respect to other variables contribution as a strong or moderate predictor.  According to the correlation plot and table, we see that bill_length_mm and body_mass_g has a moderate correlation of 0.589.  Body mass has a positive impact on length of bill.  Conversely, bill_depth_mm is negatively related to bill_length_mm and body_mass_g.  Furthermore, we see that (bill_length_mm with flipper_length_mm), (bill_length_mm with body_mass_g), (bill_depth_mm with bill_depth_mm), (flipper_length_mm with bill_depth_mm) have a moderate correlation.  We will use all of the numeric variables with the exception of Year.  Year does not provide the same level of significance as bill length, depth and body mass.

## (Q1b) KNN - Data Prep  {.tabset .tabset-fade .tabset-pills}

### Encode Categorical Variables

KNN works well when all perdictor data has been converted to numerics with the exception of the target variable (species).  

```{r}
org_penguins_df <- penguins_df
penguins_df <- penguins_df %>% select(-year) %>%
mutate_if(is.factor, as.numeric)
penguins_df$species <- org_penguins_df$species 
print(penguins_df)
```

### Normalize function

This feature is of paramount importance since the scale used for the values for each variable might be different. The best practice is to normalize the data and transform all the values to a common scale.  The scale is set between 0 and 1.

```{r}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }

penguins_df_norm <- as.data.frame(lapply(penguins_df[3:6], normalize))

#Add back other variables 
penguins_df_norm$sex <- penguins_df$sex
penguins_df_norm$island <- penguins_df$island
penguins_df_norm$species <- penguins_df$species

head(penguins_df_norm)
```

This is the final table that will be used for classification.  All of the data cleaning and manipulation has been completed.

### Create Training and Test sets

The first is used to train the system, while the second is used to evaluate the learned or trained system. In practice, the division of your data set into a test and a training sets is disjoint: the most common splitting choice is to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set.

This step created a training and test set.  The first 6 columns of the data are included in the training and test set because these are our predictors.  The last column is the target that we are predicting.

```{r}
# Create sample data for testing and training.
set.seed(1234)
ind <- sample(2, nrow(penguins_df_norm), replace=TRUE, prob=c(0.67, 0.33))

# Compose training set
penguin.train <- penguins_df_norm[ind==1, 1:6]

# Inspect training set
head(penguin.train)

# Compose test set
penguin.test <- penguins_df_norm[ind==2, 1:6]

# Inspect test set
head(penguin.test)

# Compose `penguin` training labels
penguin.trainLabels <- penguins_df_norm[ind==1,7]

# Inspect result
print(penguin.trainLabels)

# Compose `penguin` test labels
penguin.testLabels <- penguins_df_norm[ind==2, 7]

# Inspect result
print(penguin.testLabels)
```

We need to store the class labels in factor vectors and divide them over the training and test sets.  This is what was acomplished in this step.

## (Q1c) KNN - Build Classifier {.tabset .tabset-fade .tabset-pills}

### Build KNN Classifier

This step performs the actual classification.  Note: the k parameter is often an odd number to avoid ties in the voting scores. Additionally, k is usually set to the square root of the total number of observations in the dataset. 

```{r}
# Build the model
penguin_pred <- knn(train = penguin.train, test = penguin.test, cl = penguin.trainLabels, k=19)

# Inspect `penguin_pred`
penguin_pred
```

### Evaluation of Model
```{r}
# Put `penguin.testLabels` in a data frame
penguinTestLabels <- data.frame(penguin.testLabels)

# Merge `penguin_pred` and `penguin.testLabels` 
merge <- data.frame(penguin_pred, penguin.testLabels)

# Specify column names for `merge`
names(merge) <- c("Predicted Species", "Observed Species")

# Inspect `merge` 
paged_table(merge)

CrossTable(x = penguin.testLabels, y = penguin_pred, prop.chisq=FALSE)

xtab <- caret::confusionMatrix(penguin_pred, penguin.testLabels)
print(xtab)
```

This step assembles the output data to be used in a contingency table.  From this table, you can derive the number of correct and incorrect predictions. In this instance, we see that KNN classifed most/all of the test data correctly.  This can be verified by the first table where the initial vertical column named (penguin.testLabels) contains test labels and the remainder columns are the predicted values.  For Adelie, we see that it correclty predicted Adelie out of 48 observations.  The same goes for Chinstrap (22 observations) and Gentoo (34 observations).  There were not any misclassifications.  The is verified using the second table which is same the first table, but with summarized information.  In the second table, we see that Adelie was predicted correctly along with Chinstrap and Gentoo.  In addition, we see that accuracy for all three species is nearly at 100%.          

In addition, we executed KNN changing "k" parameter between each run by incrementing it by 1 each time, and the runs between 1 - 15 were at 99% accuracy, the accuracy goes to 100% for values between 16 - 19.  The last run at 20, goes back to 99% accuracy.   

## (Q1d) KNN - Appendix {.tabset .tabset-fade .tabset-pills}

### KNN Test Runs 

With k parameter adjusted with each run; total of 20 runs

K = 1  
               Accuracy : 0.99            


##############################################################################################

K = 2
             Accuracy : 0.98            


##############################################################################################

K = 3
              Accuracy : 0.99            


##############################################################################################

K = 4
              Accuracy : 0.99            


##############################################################################################

K = 5
              Accuracy : 0.99            


##############################################################################################

K = 6
              Accuracy : 0.99


##############################################################################################

K = 7
                Accuracy : 0.99   


##############################################################################################

K = 8
                Accuracy : 0.99  


##############################################################################################

K = 9
                Accuracy : 0.99    


##############################################################################################

K = 10
                Accuracy : 0.99  


##############################################################################################

K = 11
               Accuracy : 0.99   


##############################################################################################

K = 12
               Accuracy : 0.99 


##############################################################################################

K = 13
                Accuracy : 0.99  


##############################################################################################

K = 14
                Accuracy : 0.99 


##############################################################################################

K = 15
                Accuracy : 0.99 


##############################################################################################

K = 16
                Accuracy : 1


##############################################################################################

K = 17
                Accuracy : 1 


##############################################################################################

K = 18
                Accuracy : 1  


##############################################################################################

K = 19
                Accuracy : 1


##############################################################################################

K = 20
                Accuracy : 0.99 


##############################################################################################

### References

1. https://www.kdnuggets.com/2017/09/missing-data-imputation-using-r.html

2. https://www.researchgate.net/post/What-are-the-correlation-values-with-respect-to-low-moderate-high-correlation-specially-in-medical-research#:~:text=Correlation%20coefficients%20whose%20magnitude%20are,can%20be%20considered%20highly%20correlated.

3. https://rpubs.com/sediaz/Correlations

4. https://towardsdatascience.com/k-nearest-neighbors-algorithm-with-examples-in-r-simply-explained-knn-1f2c88da405c

5. https://www.datacamp.com/community/tutorials/machine-learning-in-r#normalization

6. http://r-statistics.co/Missing-Value-Treatment-With-R.html

## (Q2a) Decision Tree - EDA {.tabset .tabset-fade .tabset-pills}

### Data Exploration

Before conducting any modeling, we will explore our dataset. Below describes the `Loan_approval` dataset where we have 614 observations with 13 features used to describe these observations. eight of these variables are factors while the other five are numeric.

```{r}
url <- "https://raw.githubusercontent.com/adamleerich/data622-group4/main/hw3/Loan_approval.csv"

loandata<-read_csv(url)

summary(loandata) %>%kable() %>%   
  kable_styling(
      full_width = F, position="center", bootstrap_options = c("hover")) %>% 
  scroll_box()
```

### Missing data

We can visualize the amount of missing data that is observed in our dataset with the `vis_miss` function from the `naniar` package. Majority of the data is complete and approximate 1.9% of the dataset is missing. There are no observations with majority of features missing, and there are no features that are abnormally sparse. A threshold of 0.5 was set for observation completion, meaning if 50% of the features measure NA for any observation, that row is removed from our dataset. For this case, no rows in the dataset triggered this threshold.
The few pieces of missing information will need to be handled with imputation, removal. Of the categorical variables, this includes, `Gender`, `Married`, `Dependents`, `Self_Employed` and `Credit_History`. of the numeric variables this includes `Loan Amount`, `Loan_Amount_Terms`, and `Credit_History`. Imputation methods explored include, mean, mode, bagimpute, knn, and median.

```{r, fig.height = 9, fig.width = 12, fig.align = "center"}

naniar::vis_miss(loandata)
```

```{r}

limitmissing <- .5*ncol(loandata)

retain<-apply(loandata, MARGIN= 1, function(y) sum(length(which(is.na(y)))))<limitmissing

Dataset<-loandata[retain,]

```

### Frequency

Simple barplot is showing the frequency of all categorical variables in the dataset, as well as a histogram to show distribution of numeric features in the dataset. `Gender` comprised primarily of `Male` and this is a feature that is decidely removed from our dataset to avoid making loan approval predictions based on any gender biases. on the `Loan_Amount_Term`, most individuals go with the 30 year mortgage, while few select terms below or above that. Many of the distributions skew right, however no transformations to normalize this dataset are necessary for decision tree classification-based models.

The bar chart also depicts that there is class imbalance of the our response variable `Loan_Status`. Knowing this, we may need to perform some over/undersampling techniques in order to obtain better training sets. The table below shows this as 69% approved `Y` and 31% denied `N`. With this, we know that at the very least, guessing the majority class will provide a 69% accuracy and our models need to surpass this. Kappa is a metric used to measure accuracy that is normalized based on the class distribution and will be used to compare model performance. For Kappa, values of 0-0.20 as slight, 0.21-0.40 as fair, 0.41-0.60 as moderate, 0.61-0.80 as substantial, and 0.81-1 as almost perfect.

```{r, fig.height = 9, fig.width = 12, fig.align = "center"}
unique(loandata$Loan_Amount_Term)

plot_bar(loandata, theme_config = defaulttheme, ncol = 3)

plot_histogram(loandata,theme_config = defaulttheme, ncol = 2)

prop.table(table(loandata$Loan_Status)) %>% as.data.frame() %>% 
    mutate(Freq = paste0(round(Freq * 100), "%")) %>% 
    rename( "Class" = Var1, "Proportion" = "Freq") %>% 
    mutate(Count =table(loandata$Loan_Status)) %>%kable() %>%   
    kable_styling(
        full_width = F, position="center", bootstrap_options = c("hover"))
```
### Visualization by predicted var

Next we look at the same categorical and numeric distributions, however we separate these by our predictor variable to assess any interesting or unusual patterns. We notice that the proportion of Y/N are distributed in our categorical variables such that there is no feature that decidingly determines our prediction (which would provide less incentive to build models). However, `Credit_Score` seems to be a major determing factor.

```{r}
plot_bar(loandata, by = "Loan_Status")
plot_boxplot(loandata, by = "Loan_Status", theme_config = defaulttheme, ncol =2 )

```

Tree-based models are not susceptible to the negative impacts of colinearity, however we can still check for if there is any capacity for feature reduction by looking at our pairwise comparison of numeric features. if there are features that describe the data too similarly, they may not provide much additional benefit in the model. For this particular dataset, we do not observe heavy multicolinearity.

```{r}

plot_correlation(loandata,type = c("continuous"),cor_args = list(use = "pairwise.complete.obs"))

```


```{r}

parsnip::show_engines("decision_tree")

```

## (Q2b) Decision Tree - Model Process {.tabset .tabset-fade .tabset-pills}

### **Preprocessing**

Several models will be tested across a 10-fold bootstrapped validation training set that will allow for robust predictions on the training set that are less susceptible to overfitting. The decision tree model will be tested using the `rpart` inside of the `tidymodels` framework. Various transformations, imputations and steps are applied in each model to compare performance and is described in each model section. After testing various model setups along the validation sets, the best model will be tuned via the appropriate decision tree specifications and final predictions and accuracies will be shown on the training and testing sets which are split 75/25.


```{r}

loandata<-
  loandata %>% mutate_if(is.character,as.factor)

set.seed(3)
Datasplit<- rsample::initial_split(loandata, prop = .75, strata = Loan_Status)
Datatrain<-training(Datasplit)
datatest<-testing(Datasplit)
datacv<- vfold_cv(Datatrain, v = 10, strata = Loan_Status)

tree_engine<-
    decision_tree(mode = "classification") %>% 
    parsnip::set_engine(engine = "rpart")

dt_wf<-
    workflow() %>% 
    workflows::add_model(tree_engine)

```

### 1st Model

Workflow 1 - **Baseline** (basic imputation and feature removal)

For the initial model, start by standard steps that are necessary to make predictions including getting rid of NAs. in this case the following steps are made on this model:

- Removal of Loan_ID: unique identifier for each loan with no valuable information
- Removal of Gender: avoids gender bias
- Imputation of all numeric variables with median
- Imputation of all factor variables with mode
- Dummifying all factor variables into 0/1s depending on number of features

The results this model tested among 10-folds of a training set is 80% accuracy and 48% KAP. Other performance metrics are presented below

```{r}

#Baseline Model
dt_recipe1<-
  recipe(Loan_Status ~ ., data = Datatrain) %>% 
  step_rm(Loan_ID, Gender) %>% 
  step_medianimpute(all_numeric()) %>% 
  step_modeimpute(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes())

dt_recipe1 %>% prep()
dt_wf1<-
 dt_wf %>% 
  add_recipe(dt_recipe1)

wf1_results<-
dt_wf1 %>% fit_resamples(resamples= datacv, 
                           metrics = metric_set(roc_auc, accuracy, sensitivity, specificity,kap),
                           control = control_resamples(save_pred = T))


wf1_results %>% collect_metrics(summarize = T)

All_metrics<-data.frame()
wf_res_function<-
  function(wf_results, modelname){
    #this function calls returns the ROC_AUC, accuracy, and kap of a cross validated model result
    
    All_metrics %>% bind_rows(
      collect_metrics(wf_results, summarize = T) %>%
      mutate(model = modelname))
  }

All_metrics<-
wf_res_function(wf1_results,"Model 1")



```

### 2nd Model

Workflow 2 - **Imputation**

For this model, all steps that were applied to model 1 are also applied to model 2 except the imputation of the factor variables are done with k-nearest neighbors instead of mode imputations. The following describes the adjustments

- All steps from Model 1 except mode imputation
- Imputation of all factor variables with KNN

no change in the Accuracy or KAP was observed from model 1. Additionally bagimpute, removing missing data, and unknown classification were tested and performed either the same or worse than the baseline model.

```{r}

#model 2
dt_recipe2<-
  recipe(Loan_Status ~ ., data = Datatrain) %>% 
  step_rm(Loan_ID, Gender) %>% 
  step_medianimpute(all_numeric()) %>% 
  step_knnimpute(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes())

dt_recipe2 %>% prep()
dt_wf2<-
 dt_wf %>% 
  add_recipe(dt_recipe2)

dt_recipe2 %>% prep()

wf2_results<-
  dt_wf2 %>% fit_resamples(resamples= datacv, 
                           metrics = metric_set(roc_auc, accuracy, sensitivity, specificity,kap),
                           control = control_resamples(save_pred = T))

wf2_results %>% collect_metrics(summarize = T)



All_metrics<-
  wf_res_function(wf2_results,"Model 2")

```

### 3rd Model

Workflow 3 - **Undersampling**

All steps that were applied to model 1 are also applied to model 3 but down-sampling is added in order to test balance the majority and minority classes in our response variable. the down sampling was tested at a few ratios from 1:1 to 2. the optimal ratio is presented below at 1.8 with 35% `N` and 65% `Y`. a 4% adjustment in the class balance. the following steps were made:

- All steps from Model 1 
- Undersampling of the majority class to balance class

This resulted in a lower accuracy and lower KAP likely because this dataset is not extensive enough to provide enough information when downsampling. Due to this, Downsampling will not be incorporated as part of the final model


```{r}


#underratio 2 works
#model 3
dt_recipe3<-
  dt_recipe1 %>% 
  themis::step_downsample(Loan_Status, skip = F,under_ratio =1.8)

table((bake(dt_recipe3 %>% prep(),new_data = Datatrain))$Loan_Status)

dt_wf3<-
 dt_wf %>% 
  add_recipe(dt_recipe3)

dt_recipe3 %>% prep()

wf3_results<-
  dt_wf3 %>% fit_resamples(resamples= datacv, 
                           metrics = metric_set(roc_auc, accuracy, sensitivity, specificity,kap),
                           control = control_resamples(save_pred = T))


wf3_results %>% collect_metrics(summarize = T)


All_metrics<-
  wf_res_function(wf3_results,"Model 3")

```

### 4th Model

Workflow 4 - **Feature Reduction and Engineering**

All steps that were applied to model 1 are also applied to model 4. Additional steps were added to this model to test for feature reduction and feature engineering. `ApplicantIncome` and `CoapplicantIncome` were combined into a single variable.

- All steps from Model 1 
- Testing and removal for low variance or zero variance features
- Feature engineering - creating interation feature for`ApplicantIncome` and `CoapplicantIncome` by summation

The results of this model showed that no low-variance features exist in dataset so these steps will be removed. However, Combining income increase model KAP to 49%.

```{r}

#starting from baseline
dt_recipe4<-
  dt_recipe1 %>% 
  step_zv(all_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_mutate(Total_Income = ApplicantIncome+CoapplicantIncome) %>% 
  step_rm(ApplicantIncome, CoapplicantIncome)
  

dt_recipe4 %>% prep()

dt_wf4<-
 dt_wf %>% 
  add_recipe(dt_recipe4)

wf4_results<-
dt_wf4 %>% fit_resamples(resamples= datacv, 
                           metrics = metric_set(roc_auc, accuracy, sensitivity, specificity,kap),
                           control = control_resamples(save_pred = T))


wf4_results %>% collect_metrics(summarize = T)

All_metrics<-
wf_res_function(wf4_results,"Model 4")


```

### 5th Model

Workflow 5 - **Feature Reduction** Part 2

All steps that were applied to model 1 are also applied to model 5. Additional steps were added to this model to test for feature reduction and feature engineering. `ApplicantIncome` and `CoapplicantIncome` were combined into a single variable. The following was made to model 5:

- All steps from Model 1 
- Feature engineering - creating interaction feature for`ApplicantIncome` and `CoapplicantIncome` by summation
- Feature engineering - creating interaction feature for `Total_Income` and `LoanAmount` to create `IncomeLoanRatio`


Adding the `IncomeLoanRatio` Feature increased our model KAP from 49% to 52% and our accuracy from 80% to 82% so we will retain these steps.

```{r}

#starting from 1, removing nzv and zv, adding income to loan
#model 5
dt_recipe5<-
  dt_recipe1 %>% 
  step_mutate(Total_Income = ApplicantIncome+CoapplicantIncome) %>% 
  step_rm(ApplicantIncome, CoapplicantIncome) %>% 
  step_mutate(IncomeLoanRatio = Total_Income/LoanAmount) 

dt_recipe5 %>% prep()

dt_wf5<-
 dt_wf %>% 
  add_recipe(dt_recipe5)

wf5_results<-
dt_wf5 %>% fit_resamples(resamples= datacv, 
                           metrics = metric_set(roc_auc, accuracy, sensitivity, specificity,kap),
                           control = control_resamples(save_pred = T))


wf5_results %>% collect_metrics(summarize = T)


All_metrics<-
wf_res_function(wf5_results,"Model 5")

```

### 6th Model

Workflow 6 - **Discretization Testing**

All steps that were applied to model 5 are also applied to model 6. Additionally, Data discretization for numeric features were tested. Binning features sometimes allows groupings to better predict our response variable. Binning was tested along various features and various bins, the feature that improved model accuracy most when binning was the engineered feature `Total_Income`. The following steps were done to model 6:

- All steps from Model 5
- Discretization of `Total_Income` into 6 bins

```{r}

#starting from 5, adding to this by discretizing the total income variable
#model 5
dt_recipe6<-
  dt_recipe5 %>% 
  step_discretize(Total_Income, num_breaks = 8)
  
dt_recipe6 %>% prep
dt_wf6<-
 dt_wf %>% 
  add_recipe(dt_recipe6)

wf6_results<-
dt_wf6 %>% fit_resamples(resamples= datacv, 
                           metrics = metric_set(roc_auc, accuracy, sensitivity, specificity,kap),
                           control = control_resamples(save_pred = T))


wf6_results %>% collect_metrics(summarize = T)

All_metrics<-
wf_res_function(wf6_results,"Model 6")

```

Binning our engineered feature `Total_Income` increased our model KAP from 52% to 53% and our accuracy from 82.1% to 82.4% so we will retain these steps. 

## (Q2c) Decision Tree - Model Selection {.tabset .tabset-fade .tabset-pills}

### Selection

The average KAP metric tested on 10-validation training sets for each model is presented below and model 6 will be the model selected for further tuning and model finalization.

```{r}
All_metrics %>% filter(.metric == "kap") %>% kable() %>% 
     kable_styling(
      full_width = F, position="center", bootstrap_options = c("hover"))


```

### Model Tuning 

After optimizing our model based on various transformations accross multiple validation datasets, the best model is selected and needs to be tuned. There are various specifications of a decision tree that can affect the performance. The ones that will be tuned in this section are:

- `cost_complexity`: represents the amount of information gain required for a tree to continue splitting along a node

- `tree_depth`: represents the maximum amount of branches a tree may extend before terminating

- `min_n`: represents the minimum number of datapoints required at a node for a split to be made


In order to test various values, a grid of these features are set up with various combinations of options as shown below. 

```{r}
tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 4)
tree_grid %>%  kable %>% 
  kable_styling(
      full_width = F, position="center", bootstrap_options = c("hover")) %>%
  scroll_box(height = "200px")
```

All of these values are testing on the 10-fold cross-validation set where the result of every combination along each 10 splits of the data are averaged. The results of this tuning exercise is plotted below. KAP is the metric used to select the best tree, where the minimum cost function, a tree depth of 5 and a minimal node size of 40 is shown to be optimal.

```{r}
doParallel::registerDoParallel()

tree_engine<-
  decision_tree(mode = "classification",
                cost_complexity = tune(), #the minimum improvement in model needed at each node
                tree_depth = tune(), #The maximum depth of the tree allowed
                min_n(tune())#minimum number of datapoints required in node to tune decision tree
                ) %>% 
  set_engine(engine = "rpart")
             
set.seed(3)

tree_rs <- tune_grid(
  object =  tree_engine,
  preprocessor = dt_recipe6,
  resamples = datacv,
  grid = tree_grid,
  metrics = metric_set(accuracy, kap)
)

collect_metrics(tree_rs) %>% kable() %>% 
   kable_styling(
      full_width = F, position="center", bootstrap_options = c("hover")) %>% 
  scroll_box(height = "200px")

tree_rs %>% autoplot()+theme_light(base_family = "IBMPlexSans")


```

## (Q2d) Decision Tree - Final Results {.tabset .tabset-fade .tabset-pills}

### Finalize decision

We can finalize our decision and create our decision tree based on the following specification:

```{r}
show_best(tree_rs, "kap")

final_tree <- finalize_model(tree_engine, select_best(tree_rs, "kap"))

final_tree
```

A visual of our fitted model tree to the training dataset is shown below. The first value in the tree nodes indicate Y/N on whether there loan default would be predicted at that node, the second values shows the probability associated with `N` on the left and `Y` on the right. the third row in the node shows the percentage of the data contained in each node. The top most node depicts the most descriptive feature of our classification model, while following nodes depict secondary descriptive features. Due to the tree specifications (40 data points needed minimum at each node) only two main features are used to predict `LoanStatus` which are `Credit_History` and `IncomeLoanRatio`. 


```{r}
final_wf<-
  workflow() %>% 
  add_model(final_tree) %>% 
  add_recipe(dt_recipe6)

wf_final_results<-
final_wf %>% fit_resamples(resamples= datacv, 
                           metrics = metric_set(roc_auc, accuracy, sensitivity, specificity,kap),
                           control = control_resamples(save_pred = T))


#wf_final_results %>% collect_metrics(summarize = T)

All_metrics<-
wf_res_function(wf_final_results,"wf_final")

dtmodel<-
  final_wf %>% fit(data = Datatrain)

#rpart.plot::rpart.plot(dtmodel$fit$fit$fit)
rattle::fancyRpartPlot(dtmodel$fit$fit$fit,palettes="RdPu")

```

### Variable Importance Plot

Additional information about the variable importance was extracted from the model and shown in the variable importance plot below. where `Credit_History` has an overwhelming weight of importance, followed by our engineered feature `IncomeLoanRatio`.

```{r}
#For plotting tree
caret::varImp(dtmodel$fit$fit$fit) %>% mutate(Feature = rownames(.)) %>% 
  ggplot(mapping = aes(x =fct_reorder(Feature,Overall), y = Overall))+
  geom_col(fill ="skyblue3")+
  coord_flip()+
  defaulttheme+
  labs(x = "Importance",
       y = "Feature",
       title = "Variable Importance")

#traintransformed<-bake(dt_recipe6 %>% prep,Datatrain)
#testtransformed<-bake(dt_recipe6 %>% prep,datatest)

training_predictions<-
predict(dtmodel,Datatrain, type = "class") %>% 
  bind_cols(Datatrain$Loan_Status) %>% 
  rename("Loan_Status"= "...2") %>% 
  mutate(set = "Training")


```

### Confusion Matrix, Accuracy

The confusion matrix and accuracy of our training set is shown below with an accuracy of 83.1% and kappa of 55.6%. The confusion matrix shows that the model is not very good at predicting the minority class of when an individual will not be approved for the loan.

```{r}
training_predictions %>% conf_mat(truth = Loan_Status, estimate=.pred_class)

metrics(training_predictions, truth = Loan_Status,estimate = .pred_class)


```

Similarly, the confusion matrix and accuracy of our testing set is shown below with an accuracy of 79.7% and kappa of 46.6%. Similar to the training dataset, the confusion matrix shows that the model is not very good at predicting the minority class of when an individual will not be approved for the loan.

```{r}
testing_predictions<-
predict(dtmodel,datatest, type = "class") %>% 
  bind_cols(datatest$Loan_Status) %>% 
  rename("Loan_Status"= "...2") %>% 
  mutate(set = "Testing")

testing_predictions %>% conf_mat(truth = Loan_Status, estimate=.pred_class)

metrics(testing_predictions, truth = Loan_Status,estimate = .pred_class)

```

## (Q3a) Random Forest - Data Prep {.tabset .tabset-fade .tabset-pills}

### Import and Preparation

In Step 1, I imported the data and initially decided to eliminate the Loan_ID variable from the data because it was a contiguous variable. The data set contained several empty values (not NAs) in the categorical variables, therefore, I decided to mitigate those empty values using random replacement. Afterwards, I decided to impute the rest of the numerical variables so that I had no NAs. The presence of NAs and empty categorical values were small in the data set, but I decided to cover them so that I would not have to worry about them while I performed Random Forest operations.

```{r, message=FALSE, echo=FALSE}
hw3data = read.csv(file = 'Loan_approval.csv')
hw3temp <- as.data.frame(hw3data)
hw3temp <- hw3temp[,!(names(hw3temp) %in% c("Loan_ID"))]
summary(hw3temp)
```

### Handling of Empty Categorical Values

Based on my initial study of the data, I identified **Gender**, **Married**, **Dependents**, and **Self_Employed** categorical variables that contained 13, 3, 15, and 32 empty values respectively. Imputation nor na.omit would work with mitigating these variables, therefore, I decided to use the sample method to randomly replace the empty values of these categorical variables.

```{r, message=FALSE, echo=FALSE}
hw3temp$Gender[hw3temp$Gender == ""] <- sample(c('Male', 'Female'), 13, replace=TRUE, prob=c(0.5, 0.5))
hw3temp$Married[hw3temp$Married == ""] <- sample(c('Yes', 'No'), 3, replace=TRUE, prob=c(0.5, 0.5))
hw3temp$Dependents[hw3temp$Dependents == ""] <- sample(c('0', '1','2','3+'), 15, replace=TRUE, prob=c(0.25, 0.25,0.25,0.25))
hw3temp$Self_Employed[hw3temp$Self_Employed == ""] <- sample(c('Yes', 'No'), 32, replace=TRUE, prob=c(0.5, 0.5))
hw3temp$Gender <- factor(as.character(hw3temp$Gender))
hw3temp$Married <- factor(as.character(hw3temp$Married))
hw3temp$Dependents <- factor(as.character(hw3temp$Dependents))
hw3temp$Self_Employed <- factor(as.character(hw3temp$Self_Employed))
summary(hw3temp)
```

### Imputing Missing Values

The following variables had missing data and I used the MICE function to remediate the missing values. Perhaps, it would have been a lot easier to omit the missing data, but I felt that it would be a more truthful data set if we had imputed data values to play with. 

1. LoanAmount

2. Loan_Amount_Term

3. Credit_History

```{r, message=FALSE, echo=FALSE}
hw3imputed <- mice(hw3temp, m=5, maxit = 5, method = 'pmm')
hw3imputed <- complete(hw3imputed)
summary(hw3imputed)
```

## (Q3b) Random Forest - Full Model (without Variable Selection) {.tabset .tabset-fade .tabset-pills}

### Partition of Training/Test Data and Data Transformation

Based on my literatue review of Random Forests, it appears that the both categorical and numerical variables can be used in default Random Forests. It appears that Random Forests can be executed with all variables in place. In this execution of Random Forest, I decided to test **Loan_Status** against all remaining variables in the data set.

I used createDataPartition to develop the Training and Test data sets. I separated the training and test data set using 70/30 partition of the imputed data.

```{r}
set.seed(123)
inTrain1 <- createDataPartition(y = hw3imputed$Loan_Status, p=0.70, list = FALSE)
training1 <- hw3imputed[inTrain1,]
testing1 <- hw3imputed[-inTrain1,]
```

### Execution of Random Forest

I ran the random forest and I very disatifsfied with the results. There is an error rate of 22.51% which I need to address. This means that I may have to investigate if a reduction of variables is needed for the random forest creation. Moreover, a normal plot for a random forest should show a curve sloping down near the X and Y axes. There are 3 different curves almost parellel to each other.

```{r, message=FALSE, echo=FALSE}
set.seed(521)
m1 <- randomForest(Loan_Status ~ ., data=training1, mtry=12, importance=TRUE)
m1
importance(m1)
plot(m1)
```

## (Q3c) Random Forest - Model with 3 Most Important Variables {.tabset .tabset-fade .tabset-pills}

### Features Highlight

Upon execution of the **importance** method in the previous model, I noticed that the strongest variable candidates are: **Credit_History**, **ApplicantIncome**, and **LoanAmount** due to them having the highest Mean Decrease Gini Coeeficient. I ran a subsequent randomForest function using these variables only and saw that the error rate went up to 26.91%.

```{r, message=FALSE, echo=FALSE}
set.seed(121)
m1a <- randomForest(Loan_Status ~ Credit_History + ApplicantIncome + LoanAmount, data=training1, mtry=3, importance=TRUE)
m1a
plot(m1a)
```

The plots for model and sub-model left me a little uneasy. Therefore, I decided to run another Random Forest model with the Categorical variables transformed into numerical variables and the dealing with multicollinear variables.

## (Q3d) Random Forest - Variable Selection via Determining Highly-Correlated Variables {.tabset .tabset-fade .tabset-pills}

### Determining Highly-Correlated Variables

During my research into Random Forests, I have seen conflicting literature about the handling of multicollinear and numerical transformation of categorical variables. Some literature indicate it is unnecessary to numerically transform your categorical variables and others indicate otherwise. Moreover, the literature was not conclusive in determining if multicollinearity variable will affect the model. In this execution of the Random Forest model, I decided to explore both avenues to determine if I could find a model that would be satisfactory.

To determine and eliminate a highly-correlated variable from the model, may improve the fitness of the models.

Prior to executing the cor Pearson function, I had to transform the temporary categorical data variables into numerical equivalents. Category variables that needed to be transformed were **Loan_Status**, **Married**, **Self_Employed**, **Education**, **Gender**, **Dependents**, and **Property_Area** variables.

```{r, message=FALSE, echo=FALSE}
hw3temp = hw3imputed
hw3temp$Loan_Status = case_when(hw3temp$Loan_Status == 'N' ~ 0, 
                                         hw3temp$Loan_Status == 'Y' ~ 1)
hw3temp$Married = case_when(hw3temp$Married == 'No' ~ 0, 
                                         hw3temp$Married == 'Yes' ~ 1)
hw3temp$Self_Employed = case_when(hw3temp$Self_Employed == 'No' ~ 0, 
                                         hw3temp$Self_Employed == 'Yes' ~ 1)
hw3temp$Education = case_when(hw3temp$Education == 'Not Graduate' ~ 0, 
                                         hw3temp$Education == 'Graduate' ~ 1)
hw3temp$Gender = case_when(hw3temp$Gender == 'Female' ~ 0, 
                                         hw3temp$Gender == 'Male' ~ 1)
hw3temp$Dependents = case_when(hw3temp$Dependents == '0' ~ 0,
                                         hw3temp$Dependents == '1' ~ 1,
                                         hw3temp$Dependents == '2' ~ 2,
                                         hw3temp$Dependents == '3+' ~ 3)
hw3temp$Property_Area = case_when(hw3temp$Property_Area == 'Rural' ~ 0,
                                         hw3temp$Property_Area == 'Semiurban' ~ 1,
                                         hw3temp$Property_Area == 'Urban' ~ 2)
summary(hw3temp)
```

I conducted a cor function using the Pearson method to find out of any correlations in the data. It was determined that only **Credit_History** had high positive correlation in 0.56. However, this number is in the midpoint in the positive correlation and is not very close to 1. Being close to 1 after the 0.80 range would convince me that this is a multicollinear variable. 

```{r, message=FALSE, echo=FALSE}
relationship <- cor(hw3temp, method = "pearson", use = "complete.obs")
kable(relationship, booktabs = 'T') %>% 
  kable_styling(font_size = 8)
corrplot(relationship, order = "original",tl.col = "black",method="circle")

# correlations cont.
temp1 <- as.data.frame(relationship) %>% dplyr::select(Loan_Status)

# Looking for very highly correlated, arbitrarily set at > .5 or < -.5
correlated_pos <- subset(temp1, temp1[,'Loan_Status'] > .5 & temp1[,'Loan_Status'] < 1)
correlated_neg <- subset(temp1, temp1[,'Loan_Status'] < -.5 & temp1[,'Loan_Status'] > -1)
```

**Positive Correlated Variables** 

```{r, message=FALSE, echo=FALSE}
correlated_pos
```

**Negative Correlated Variables** 

```{r, message=FALSE, echo=FALSE}
correlated_neg
```

Based on a lecture from Professor Chris Mack, one way to determine which variable to eliminate from the model is to find the variables with high Variable Inflation Factor [MAC]. According to Professor Mack, if a variable has a high VIF, greater than 5, then we can determine that variable as a candidate of elimination due to its high inflation effect on other variables. His strategy involves an execution of a linear regression model against the data and use the vif method to determine which variables to eliminate.

```{r, message=FALSE, echo=FALSE}
lmmodel <- lm(Loan_Status~.,data=hw3temp)
summary(lmmodel)
vif(lmmodel)
```

Upon running the **vif** function, you will see that **ApplicantIncome** and **LoanAmount** have the highest inflation factor values at 1.65 and 1.74 respectively. Notice that **Credit_History** as in the lower threshhold of numbers to eliminate. Observe in the correlation map above, **ApplicantIncome** and **LoanAmount** are within a light blue color range indicating that they have multicollinearity but below the 0.5 mark. According to Professor Mack, he indicated that any VIF values greater than 5 should be candidate variables for elimination. Given these indications, I am not eliminating any variables from the Random Forest model.

Again, I used createDataPartition to develop the Training and Test data sets. I separated the training and test data set using 70/30 partition of the imputed data.

```{r}
set.seed(123)
inTrain2 <- createDataPartition(y = hw3temp$Loan_Status, p=0.70, list = FALSE)
training2 <- hw3temp[inTrain2,]
testing2 <- hw3temp[-inTrain2,]
```

### Execution of Random Forest

I ran the random forest and I am still disatifsfied with the results. There is no error rate to be concerned with, but only 32.25% of the variance can be explained in the model. A percentage close to 100% would make me feel confident for the Random Forest model. The plot, however, follows the curve of ideal Random Forests [JAM].

```{r}
m2 <- randomForest(Loan_Status ~ ., data=training2, mtry=11, importance=TRUE)
m2
importance(m2)
plot(m2)
```

### Model with 3 Most Important Variables Using Transformed Data

Upon execution of the **importance** method in the previous model, I noticed that the strongest variable candidates AGAIN are: **Credit_History**, **ApplicantIncome**, and **LoanAmount** due to them having the highest Mean Decrease Gini Coeeficient (IncNodePurity). I ran a subsequent randomForest function using these variables only and saw that while there was no error rate observed, the percent of variance explained actually went down to the low 20s. As indicated earlier, a percentage close to 100% would make me feel confident for the Random Forest model. The plot, however, follows the curve of ideal Random Forests.

```{r, message=FALSE, echo=FALSE}
set.seed(123)
m2a <- randomForest(Loan_Status ~ Credit_History + ApplicantIncome + LoanAmount, data=training2, mtry=3, importance=TRUE)
m2a
plot(m2a)
```

## (Q3e) Random Forest - Model Comparison {.tabset .tabset-fade .tabset-pills}

In the final chore for this exercise, I will make predictions on all four generated random forest models and develop the confusion matrix for each. I will extract important values from these models and compare and discuss them.

### Model 1

```{r, message=FALSE, echo=FALSE}
prediction1 <- predict(m1,newdata = testing1)
prediction1.cm <- confusionMatrix(prediction1, testing1$Loan_Status) 
prediction1.cm
```

### Model 1a

```{r, message=FALSE, echo=FALSE}
prediction1a <- predict(m1a,newdata = testing1)
prediction1a.cm <- confusionMatrix(prediction1a, testing1$Loan_Status) 
prediction1a.cm
```

### Model 2

```{r, message=FALSE, echo=FALSE}
prediction2 <- factor(round(predict(m2, testing2, type='response')), levels=c('0', '1'))
prediction2.cm <- confusionMatrix(data=prediction2, reference=factor(testing2$Loan_Status, levels=c('0', '1'))) 
prediction2.cm
```

### Model 2a

```{r, message=FALSE, echo=FALSE}
prediction2a <- factor(round(predict(m2a, testing2, type='response')), levels=c('0', '1'))
prediction2a.cm <- confusionMatrix(data=prediction2a, reference=factor(testing2$Loan_Status, levels=c('0', '1'))) 
prediction2a.cm
```

### Final Comparison

- Regardless of variable selection or transformation of variables, the initial full Random Forest Model (Model 1) has the highest accuracy at 80% and F1-Score at 79%. Despite the high error rate indicated earlier, F1-Score and Accuracy seems to indicate that this Random Forest is an optimal model.

- Manual selection of the optimal variables according to **vif** and multicollinear mitigation did not result in a better Random Forest model. 

- Despite the numerical transformation of the categorical variables, the initial full Random Forest Model was (Model 1) was better than the full Random Forest Model with transformed data (Model 2).

- By visual inspection of the confusion matrices, the p-value of the initial full Random Forest Model is signficantly less than 0.05 at 0.0001763. Model 2's p-value is 0.02. These indicate that both models are valid models. Models 1a and 2a have p-values of 0.086 and 0.41 respectively indicating that there is evidence that they are not valid models since their p-values are greater than 0.05.

```{r, message=FALSE, echo=FALSE}
# Model 1 Values
prediction1.accuracy <- prediction1.cm$overall['Accuracy']
prediction1.TN <- prediction1.cm$table[1,1]
prediction1.FP <- prediction1.cm$table[1,2]
prediction1.FN <- prediction1.cm$table[2,1]
prediction1.TP <- prediction1.cm$table[2,2]
prediction1.TPR <- prediction1.TP /(prediction1.TP + prediction1.FN)
prediction1.TNR <- prediction1.TN /(prediction1.TN + prediction1.FP)
prediction1.FPR <- prediction1.FP /(prediction1.TN + prediction1.FP)
prediction1.FNR <- prediction1.FN /(prediction1.TP + prediction1.FN)
prediction1.precision <- prediction1.TP / (prediction1.TP + prediction1.FP)
prediction1.recall <- prediction1.TP / (prediction1.TP + prediction1.FN)
prediction1.specificity <- prediction1.TN / (prediction1.TN + prediction1.FP)
prediction1.f1score <- 2 * ((prediction1.precision * prediction1.recall) / (prediction1.precision + prediction1.recall))
```

```{r, message=FALSE, echo=FALSE}
# Model 1a Values
prediction1a.accuracy <- prediction1a.cm$overall['Accuracy']
prediction1a.TN <- prediction1a.cm$table[1,1]
prediction1a.FP <- prediction1a.cm$table[1,2]
prediction1a.FN <- prediction1a.cm$table[2,1]
prediction1a.TP <- prediction1a.cm$table[2,2]
prediction1a.TPR <- prediction1a.TP /(prediction1a.TP + prediction1a.FN)
prediction1a.TNR <- prediction1a.TN /(prediction1a.TN + prediction1a.FP)
prediction1a.FPR <- prediction1a.FP /(prediction1a.TN + prediction1a.FP)
prediction1a.FNR <- prediction1a.FN /(prediction1a.TP + prediction1a.FN)
prediction1a.precision <- prediction1a.TP / (prediction1a.TP + prediction1a.FP)
prediction1a.recall <- prediction1a.TP / (prediction1a.TP + prediction1a.FN)
prediction1a.specificity <- prediction1a.TN / (prediction1a.TN + prediction1a.FP)
prediction1a.f1score <- 2 * ((prediction1a.precision * prediction1a.recall) / (prediction1a.precision + prediction1a.recall))
```

```{r, message=FALSE, echo=FALSE}
# Model 2 Values
prediction2.accuracy <- prediction2.cm$overall['Accuracy']
prediction2.TN <- prediction2.cm$table[1,1]
prediction2.FP <- prediction2.cm$table[1,2]
prediction2.FN <- prediction2.cm$table[2,1]
prediction2.TP <- prediction2.cm$table[2,2]
prediction2.TPR <- prediction2.TP /(prediction2.TP + prediction2.FN)
prediction2.TNR <- prediction2.TN /(prediction2.TN + prediction2.FP)
prediction2.FPR <- prediction2.FP /(prediction2.TN + prediction2.FP)
prediction2.FNR <- prediction2.FN /(prediction2.TP + prediction2.FN)
prediction2.precision <- prediction2.TP / (prediction2.TP + prediction2.FP)
prediction2.recall <- prediction2.TP / (prediction2.TP + prediction2.FN)
prediction2.specificity <- prediction2.TN / (prediction2.TN + prediction2.FP)
prediction2.f1score <- 2 * ((prediction2.precision * prediction2.recall) / (prediction2.precision + prediction2.recall))
```

```{r, message=FALSE, echo=FALSE}
# Model 2a Values
prediction2a.accuracy <- prediction2a.cm$overall['Accuracy']
prediction2a.TN <- prediction2a.cm$table[1,1]
prediction2a.FP <- prediction2a.cm$table[1,2]
prediction2a.FN <- prediction2a.cm$table[2,1]
prediction2a.TP <- prediction2a.cm$table[2,2]
prediction2a.TPR <- prediction2a.TP /(prediction2a.TP + prediction2a.FN)
prediction2a.TNR <- prediction2a.TN /(prediction2a.TN + prediction2a.FP)
prediction2a.FPR <- prediction2a.FP /(prediction2a.TN + prediction2a.FP)
prediction2a.FNR <- prediction2a.FN /(prediction2a.TP + prediction2a.FN)
prediction2a.precision <- prediction2a.TP / (prediction2a.TP + prediction2a.FP)
prediction2a.recall <- prediction2a.TP / (prediction2a.TP + prediction2a.FN)
prediction2a.specificity <- prediction2a.TN / (prediction2a.TN + prediction2a.FP)
prediction2a.f1score <- 2 * ((prediction2a.precision * prediction2a.recall) / (prediction2a.precision + prediction2a.recall))
```

```{r, echo=FALSE}
Model <- c("RF1","RF1A", "RF2", "RF2A")
Accuracy <- c(prediction1.accuracy, prediction1a.accuracy, prediction2.accuracy,prediction2a.accuracy)
Recall <- c(prediction1.recall, prediction1a.accuracy, prediction2.recall,prediction2a.recall)
Specificity <- c(prediction1.specificity, prediction1a.specificity, prediction2.specificity,prediction2a.specificity)
Precision <- c(prediction1.precision, prediction1a.precision, prediction2.precision,prediction2a.precision)
F1Score <- c(prediction1.f1score, prediction1a.f1score, prediction2.f1score,prediction2a.f1score)
TPR <- c(prediction1.TPR, prediction1a.TPR, prediction2.TPR, prediction2a.TPR)
TNR <- c(prediction1.TNR, prediction1a.TNR, prediction2.TNR, prediction2a.TNR)
FPR <- c(prediction1.FPR, prediction1a.FPR, prediction2.FPR, prediction2a.FPR)
FNR <- c(prediction1.FNR, prediction1a.FNR, prediction2.FNR, prediction2a.FNR)

tableModel <- data.frame(Model,Accuracy,Recall,Specificity,Precision,F1Score,TPR,TNR,FPR,FNR)
tableModel %>%
  kable() %>%
  kable_styling()
```

### References

[JAM] G. James, D. Witten, T. Hastie, R. Tibshirani. An Introduction to Statistical Learning : with Applications in R. New York: Springer, 2013.

[MAC] C. Mack. Lecture52 (Data2Decision) Detecting Multicollinearity in R. Retrieved from website: https://www.youtube.com/watch?v=QruEcbgfhzo

[RAN] Random Forests. Retrieved from website: https://uc-r.github.io/random_forests

[RRA] R Random Forest Tutorial with Example. Retrieved from website: https://www.guru99.com/r-random-forest-tutorial.html

