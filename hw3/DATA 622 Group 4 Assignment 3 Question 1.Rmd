---
title: "DATA 622 Group 4 Assignment 3"
author: "Ajay Arora"
date: "3/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, warning = FALSE, message = FALSE}
library(palmerpenguins) # access to data
#library(tibble)    
#library(car)
#library(MASS)           # fitting lda and qda
#library(klaR)           # fitting naive bayes
#library(knitr)          
#library(kableExtra)
#library(mice)
library(htmlTable)
library(gmodels)
library (class)
library(dplyr)
library(corrplot)

```

## R Markdown

# Problem #1

Please use K-nearestneighbor (KNN) algorithm to predict the species variable.Please be sure to walk through the steps you took.(40points)


## What is kNN Algorithm?
Let’s assume we have several groups of labeled samples. The items present in the groups are homogeneous in nature. Now, suppose we have an unlabeled example which needs to be classified into one of the several labeled groups. How do you do that? Unhesitatingly, using kNN Algorithm.

k nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. This algorithms segregates unlabeled data points into well defined groups.


# DATA EXPLORATION

## Data Type & Format

A tibble with 344 rows and 8 variables:
species

    a factor denoting penguin species (Adélie, Chinstrap and Gentoo)
island

    a factor denoting island in Palmer Archipelago, Antarctica (Biscoe, Dream or Torgersen)
bill_length_mm

    a number denoting bill length (millimeters)
bill_depth_mm

    a number denoting bill depth (millimeters)
flipper_length_mm

    an integer denoting flipper length (millimeters)
body_mass_g

    an integer denoting body mass (grams)
sex

    a factor denoting penguin sex (female, male)
year

    an integer denoting the study year (2007, 2008, or 2009)

## Summary of penguin data

```{r}
summary(penguins)
```
## Discussion
  The summary table provides some simple statistics of the penguin data.  E.g., we see a total number per species (Adelie, Chinstrap & Gentoo) as well as the number on each different island.  In addition, we that each island has a certain number of penguins present. Furthermore, we that each of the quatifiable variables (bill_length_mm, bill_depth_mm, flipper_length_mm, & body_mass_g) shows distribution of the numbers. We have a minimum, maximum, average, median, etc.  In addition, we also see a number of missing values.  Lastly, we  see number of species by sex distribution and the range of years in the which the data was obtained.   


## Mitigate any missing data.
```{r}
penguins_df = na.omit(penguins)
summary(penguins_df)
```

## Discussion
  We have opted to remove missing data observations versus impute the missing values.  For numerical data, the mean would not change if we imputed, but, variance decreases and changes.(1) The 4 variables (bill_length_mm  bill_depth_mm   flipper_length_mm  body_mass_g) all have the same number (2) of missing data.  By removing these observations, we are treating all of these variables in the same manner (equally), which does not introduce bias.  Lastly, variable sex has 11 missing data points.  This was removed because the number of observations between female and male are roughly the same number.  If we attempted to artifically account for the missing data points, we could be adding bias.  Given, it is a small dataset, most of the data points are intact and we don't believe that the model will lose power during modeling.  If we have added values artifically, we would add bias because it is a small dataset and this would lead to a magnified variance.  We opted to keep the data integrity pure as possible. (6)  


## Correlation between numerical variables

### Note: We are using the following definitions for variable correlation. For this test the correlation threshold is set at 0.5. 

Correlation coefficients whose magnitude are between 0.9 and 1.0 indicate variables which can be considered very highly correlated. Correlation coefficients whose magnitude are between 0.7 and 0.9 indicate variables which can be considered highly correlated. Correlation coefficients whose magnitude are between 0.5 and 0.7 indicate variables which can be considered moderately correlated. Correlation coefficients whose magnitude are between 0.3 and 0.5 indicate variables which have a low correlation. Correlation coefficients whose magnitude are less than 0.3 have little if any (linear) correlation. (2)


```{r}
# Select numericals
penNumeric <- penguins_df %>% select(-species, -sex, -island)
# Cor-relation between numerics
corrplot(cor(penNumeric), type = 'lower', diag = FALSE)

result1 <- cor(subset(penguins_df, select = c("bill_length_mm", "bill_depth_mm", "body_mass_g", "year")))
result1


corr_check_with_variables <- function(data, t){
  m <- cor(data)
  

  for (i in 1:nrow(m)){
    c <-  which((abs(m[i,i:ncol(m)]) > t) & (m[i,i:ncol(m)] != 1))
  
    if(length(c)> 0){
      lapply(c,FUN =  function(x) (cat(paste(colnames(data)[i], "with",colnames(data)[x]), "\n")))
     
    }
  }
}
#(3)

corr_check_with_variables(penNumeric, 0.5)

```

## Discussion
  In this step, we decided to remove Year predictor from classification.  This was done because of the low contribution of Year with respect to other variables contribution as a strong or moderate predictor.  According to the correlation plot and table, we see that bill_length_mm and body_mass_g has a moderate correlation of 0.589.  Body mass has a positive impact on length of bill.  Conversely, bill_depth_mm is negatively related to bill_length_mm and body_mass_g.  Furthermore, we see that (bill_length_mm with flipper_length_mm), (bill_length_mm with body_mass_g), (bill_depth_mm with bill_depth_mm), (flipper_length_mm with bill_depth_mm) have a moderate correlation.  We will use all of the numeric variables with the exception of Year.  Year does not provide the same level of significance as bill length, depth and body mass.
  
  
## Encode Categorical Variables

```{r}
org_penguins_df <- penguins_df
penguins_df <- penguins_df %>% select(-year) %>%
mutate_if(is.factor, as.numeric)
penguins_df$species <- org_penguins_df$species 
print(penguins_df)
```
## Discussion
  KNN works well when all perdictor data has been converted to numerics with the exception of the target variable (species).  


## Normalize function
```{r}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }

```
## Discussion 
  This feature is of paramount importance since the scale used for the values for each variable might be different. The best practice is to normalize the data and transform all the values to a common scale.  The scale is set between 0 and 1.


```{r}

#penguins_df <- as.data.frame(lapply(penguins_df[,c(1,2,3,4)], normalize))
penguins_df_norm <- as.data.frame(lapply(penguins_df[3:6], normalize))
#penguins_df_norm <- penguins_df_norm %>% select(-Year) 

#Add back other variables 
penguins_df_norm$sex <- penguins_df$sex
penguins_df_norm$island <- penguins_df$island
penguins_df_norm$species <- penguins_df$species

head(penguins_df_norm)

```
## Discussion
  This is the final table that will be used for classification.  All of the data cleaning and manipulation has been completed.  


## Create sample data for testing and training.
```{r}
set.seed(1234)
ind <- sample(2, nrow(penguins_df_norm), replace=TRUE, prob=c(0.67, 0.33))

```
## Discussion
  The first is used to train the system, while the second is used to evaluate the learned or trained system. In practice, the division of your data set into a test and a training sets is disjoint: the most common splitting choice is to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set.
  

## Create Training and Test sets
```{r}
# Compose training set
penguin.train <- penguins_df_norm[ind==1, 1:6]

# Inspect training set
head(penguin.train)

# Compose test set
penguin.test <- penguins_df_norm[ind==2, 1:6]

# Inspect test set
head(penguin.test)

```
## Discussion
  This step created a training and test set.  The first 6 columns of the data are included in the training and test set because these are our predictors.  The last column is the target that we are predicting.  

## Create Training and Test Labels
```{r}
# Compose `penguin` training labels
penguin.trainLabels <- penguins_df_norm[ind==1,7]

# Inspect result
print(penguin.trainLabels)

# Compose `penguin` test labels
penguin.testLabels <- penguins_df_norm[ind==2, 7]

# Inspect result
print(penguin.testLabels)


```
## Discussion
  We need to store the class labels in factor vectors and divide them over the training and test sets.  This is what was acomplished in this step.

  
## Build KNN Classifier
```{r}

# Build the model
penguin_pred <- knn(train = penguin.train, test = penguin.test, cl = penguin.trainLabels, k=19)

# Inspect `penguin_pred`
penguin_pred

```
## Discussion
  This step performs the actual classification.  Note: the k parameter is often an odd number to avoid ties in the voting scores. Additionally, k is usually set to the square root of the total number of observations in the dataset. 


## Evaluation of Model
```{r}

# Put `penguin.testLabels` in a data frame
penguinTestLabels <- data.frame(penguin.testLabels)

# Merge `penguin_pred` and `penguin.testLabels` 
merge <- data.frame(penguin_pred, penguin.testLabels)

# Specify column names for `merge`
names(merge) <- c("Predicted Species", "Observed Species")

# Inspect `merge` 
merge

CrossTable(x = penguin.testLabels, y = penguin_pred, prop.chisq=FALSE)

xtab <- caret::confusionMatrix(penguin_pred, penguin.testLabels)
print(xtab)


```
## Discussion 
  This step assembles the output data to be used in a contingency table.  From this table, you can derive the number of correct and incorrect predictions. In this instance, we see that KNN classifed most/all of the test data correctly.  This can be verified by the first table where the initial vertical column named (penguin.testLabels) contains test labels and the remainder columns are the predicted values.  For Adelie, we see that it correclty predicted Adelie out of 48 observations.  The same goes for Chinstrap (22 observations) and Gentoo (34 observations).  There were not any misclassifications.  The is verified using the second table which is same the first table, but with summarized information.  In the second table, we see that Adelie was predicted correctly along with Chinstrap and Gentoo.  In addition, we see that accuracy for all three species is nearly at 100%.          

  In addition, we executed KNN changing "k" parameter between each run by incrementing it by 1 each time, and the runs between 1 - 15 were at 99% accuracy, the accuracy goes to 100% for values between 16 - 19.  The last run at 20, goes back to 99% accuracy.     

## Appendix

### KNN Test Runs with k parameter adjusted with each run; total of 20 runs
K = 1  
               Accuracy : 0.99            


##############################################################################################

K = 2
             Accuracy : 0.98            


##############################################################################################

K = 3
              Accuracy : 0.99            


##############################################################################################

K = 4
              Accuracy : 0.99            


##############################################################################################

K = 5
              Accuracy : 0.99            


##############################################################################################

K = 6
              Accuracy : 0.99


##############################################################################################

K = 7
                Accuracy : 0.99   


##############################################################################################

K = 8
                Accuracy : 0.99  


##############################################################################################

K = 9
                Accuracy : 0.99    


##############################################################################################

K = 10
                Accuracy : 0.99  


##############################################################################################

K = 11
               Accuracy : 0.99   


##############################################################################################

K = 12
               Accuracy : 0.99 


##############################################################################################

K = 13
                Accuracy : 0.99  


##############################################################################################

K = 14
                Accuracy : 0.99 


##############################################################################################

K = 15
                Accuracy : 0.99 


##############################################################################################

K = 16
                Accuracy : 1


##############################################################################################

K = 17
                Accuracy : 1 


##############################################################################################

K = 18
                Accuracy : 1  


##############################################################################################

K = 19
                Accuracy : 1


##############################################################################################

K = 20
                Accuracy : 0.99 


##############################################################################################







References:

1. https://www.kdnuggets.com/2017/09/missing-data-imputation-using-r.html

2. https://www.researchgate.net/post/What-are-the-correlation-values-with-respect-to-low-moderate-high-correlation-specially-in-medical-research#:~:text=Correlation%20coefficients%20whose%20magnitude%20are,can%20be%20considered%20highly%20correlated.

3. https://rpubs.com/sediaz/Correlations

4. https://towardsdatascience.com/k-nearest-neighbors-algorithm-with-examples-in-r-simply-explained-knn-1f2c88da405c

5. https://www.datacamp.com/community/tutorials/machine-learning-in-r#normalization

6. http://r-statistics.co/Missing-Value-Treatment-With-R.html

