---
title: "Question #4"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE)
library(tidyverse)
library(xgboost)
```





## Question #4: gradient boosting

We are using the `xgboost` package to answer this question.
The `xgboost` package is incredibly powerful and flexible.
You can learn more about it and the underlying algorithms here:

* [Rachael Tatman's tutorial on Kaggle](https://www.kaggle.com/rtatman/machine-learning-with-xgboost-in-r)
* [`xgboost` page on readthedocs.io](https://xgboost.readthedocs.io/en/latest/index.html)

We followed the general steps in the tutorial to answer this question
as it is our first time using the `xgboost` package.

1. Prepare the data
2. Run a sample model
3. Tune parameters to select a model with high accuracy and low chance of over-fitting






### Prepare the data

`xgboost` requires data to be in a particular format.
The following code shows how to start with the output from the data prep
section above and obtain the desired format for the algorithm.


```{r}
# Load Rds files from data prep section
loans_train     <- readr::read_rds('data/loans_train.Rds')
loans_test      <- readr::read_rds('data/loans_test.Rds')
loans_dv_train  <- readr::read_rds('data/loans_dv_train.Rds')
loans_dv_test   <- readr::read_rds('data/loans_dv_test.Rds')

# xgboost wants predictors and response in separate objects
train_labels <- loans_dv_train$Loan_Status.Y
test_labels <- loans_dv_test$Loan_Status.Y

# We will use this to tell the function to balance the Y/N values
negative_cases <- sum(train_labels == 0)
postive_cases <- sum(train_labels == 1)

# TODO why do we need this?
loans_dv <- rbind(loans_dv_test, loans_dv_train)
mloans <- as.matrix(loans_dv)

# xgboost can use a special xgb.DMatrix object
dtrain <- loans_dv_train %>% 
  select(-Loan_Status.N, -Loan_Status.Y) %>% 
  as.matrix %>% 
  xgb.DMatrix(
    data = .,
    label = loans_dv_train$Loan_Status.Y %>% as.matrix)

# create the same for test data for the predict function
dtest <- loans_dv_test %>% 
  select(-Loan_Status.N, -Loan_Status.Y) %>% 
  as.matrix %>% 
  xgb.DMatrix(
    data = .,
    label = loans_dv_test$Loan_Status.Y %>% as.matrix)
```







### Create a single pre-tuning model

It is always good when using a new method to start with a simple
model to make sure everything is working properly.

The following function call includes the smallest number of non-default
parameters.  (In reality, `scale_pos_weight` is not required,
but our labels are slightly imbalanced, so we include it as *mandatory*.)

```{r}
xgb1 <- xgboost(
  data = dtrain,
  objective = 'binary:logistic',
  eval_metric = 'error',
  scale_pos_weight = negative_cases/postive_cases,
  nrounds = 10)
```

The training error will *always* decrease if we increase the number of iterations.
However, the change in error tends to zero.
The important question is what does the test error look like with each
successive tree?

```{r}
# Some helper functions
test_error <- function(model, n) {
  pred <- predict(model, newdata = dtest, ntreelimit = n)
  mean(as.numeric(pred > 0.5) != test_labels)
}

test_errors <- function(model) {
  N <- model$niter
  errors <- numeric(N)
  for (i in 1:N) {
    errors[i] <- test_error(model, i)
  }
  return(errors)
}

compare_errors <- function(model) {
  df <- cbind(
    model$evaluation_log %>% as.data.frame(),
    test_errors(model))
  names(df) <- c('iter', 'train_error', 'test_error')
  return(df)
}

# xgb1 errors
# TODO add kable formatting
compare_errors(xgb1)
```

Even at iteration #1 we have evidence of over-fitting.
And the test error increases on average as we
add more trees.  This is most likely because the
defaults of `xgboost` allow for a max tree depth of six!
Our selected decision model only had a depth of two,
so we probably have over-fitting after just one iteration.






### Parameter tuning

There are dozens of parameters one can pass to the `xgboost` function,
and we will not attempt to address them all here.
The most important for our first foray into `xgboost` are probably the following

* `eta` the scaling factor applied to each iteration, default is 0.3
* `max_depth` controls the maximum size of each tree, default is 6
* `nround` is the number of iterations to run
* `early_stopping_rounds` controls whether to stop before reaching `nrounds`
* `gamma` is described best by the help page: "minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be."


Through experimentation not included in this write-up,
we found that the small size of our data set means we do not need
many boosting rounds to hit a plateau in the training accuracy.
Therefore, `gamma` is probably not something we need to worry about.
(`gamma` will help prevent too many iterations that have little overall impact.)
The same argument applies to `early_stopping_rounds`.

The two arguments that make the most sense for us to tune are

* `max_depth` and
* `nround`







### Grid search

We run a grid search for the best combination of `max_depth`
and `nround` by letting `max_depth` range from 1 to 5 (we already know 6 is not good)
and `nround` range from 1 to 20.
In practice, we set `nround` to 20 and only run the algorithm 5 times:
once we have a 20-iteration result, we can use a subset of those iterations
to accomplish the search.

```{r}
error_comps <- NULL

for (d in 1:5) {
  
  # verbose = 0 suppresses interactive output
  mod_d <- xgboost(
    data = dtrain,
    objective = 'binary:logistic',
    eval_metric = 'error',
    scale_pos_weight = negative_cases/postive_cases,
    nrounds = 20,
    max_depth = d,
    verbose = 0) 
  
  error_comps_d <- compare_errors(mod_d)
  error_comps_d$max_depth <- d
  
  if (is.null(error_comps)) {
    error_comps <- error_comps_d
  } else {
    error_comps <- rbind(error_comps, error_comps_d)
  }
}
```

For each value of `max_depth` the best test errors are shown here:

```{r}
# TODO kable-fy
error_comps %>% 
  group_by(max_depth) %>% 
  summarize(min_test_error = min(test_error))
```

At `max_depth` of three, here are the results:

```{r}
error_comps %>%
  filter(max_depth == 3) %>% 
  select(-max_depth) %>% 
  mutate(
    train_error_counts = round(train_error * nrow(dtrain), 0),
    test_error_counts = round(test_error * nrow(dtest), 0))
```

It is natural for our errors to move in a choppy way
with such a small data set in a classification problem.
With a larger data set we would see smoother patterns
and a more convincing *minimum* point for our test errors.
What we do see with our model is that at iteration 17 we
finally get three more observations classified correctly 
and a low point for our test error.







### Selected parameters

The final model we selected is

```{r}
xgb_final <- xgboost(
  data = dtrain,
  objective = 'binary:logistic',
  eval_metric = 'error',
  scale_pos_weight = negative_cases/postive_cases,
  nrounds = 17,
  max_depth = 3,
  verbose = 0)
```

Seventeen may seem lot a lot of iterations, but remember each successive
tree's contribution to the whole is only 30% of its predecessor's.
The first tree carries the most weight by far.  In fact, the last sixteen
trees together have a weight less than half that of the first tree.
It might be helpful to add up all the weights to get a comparative number
of "fully-weighted" trees:

```{r}
.3^(0:16) %>% sum
```

Our seventeen trees have the same "weight" as a 1.5-tree model
with an `eta` of 1.0, if that makes any sense!






### Interpretation

Boosted trees are not as easy to interpret as single trees,
but there are still some things that are helpful.  Like a variable importance
plot:

```{r}
importance_matrix <- xgb.importance(model = xgb_final)
xgb.plot.importance(importance_matrix)
```

The rank of importance agrees with our single-tree analysis, as we'd expect.





```{r eval = FALSE}
# Save final model and predictions for Jimmy's work on #5
readr::write_rds(xgb_final, 'output/xgb-final-model.Rds')
pred_final <- predict(xgb_final, newdata = dtest)
readr::write_rds(pred_final, 'output/xgb-final-predictions.Rds')
```
