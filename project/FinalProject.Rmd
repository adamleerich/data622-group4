---
title: "Group 4 Final Project"
author: "Ajay Arora, Romerl Elizes, Jimmy Ng, Joshua Registe, Adam Rich"
date: "May 23, 2021"
output:
  rmdformats::readthedown:
    self_contained: yes
    thumbnails: yes
    lightbox: yes
    gallery: no
    highlight: tango
    code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE,
  cache = FALSE)
packages <- c(
  'tidyverse', 
  'corrplot', 
  'palmerpenguins',
  'class',
  'kableExtra',
  'naniar',
  'mice',
  'DataExplorer',
  'caret',
  'tidymodels',
  'rsample',
  'themis',
  'randomForest',
  'car',
  'xgboost',
  'broom',
  'DBI',
  'RMySQL',
  'RJDBC',
  'lattice',
  'ggplot2',
  'e1071'
)
for (pkg in packages) {
  suppressPackageStartupMessages(suppressWarnings(
    library(
      pkg, character.only = TRUE, 
      warn.conflicts = FALSE, quietly = TRUE)
  ))
}

# A ggplot2 function
defaulttheme <- theme(
  panel.background = element_blank(),
  panel.border = element_rect(color = "black", fill = NA))
```
 
## (1) Data Prep and Mitigation {.tabset .tabset-fade .tabset-pills} 

#### Introduction

After brainstorming the data sets available, Group 4 decided to use the New York City (NYC) 311 data found in the link: https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9. It is a very ambitious undertaking given that all Assignments covered in this course consisted of no more than 614 rows of records. The NYC data consists of 25+ million rows and 41 columns.
The initial goal of the project is to develop a prediction algorithm from 3 possible models that would allow 311 operators to give an estimate of when the request will be resolved?  It's a simple statement, but a complicated problem.  If the 311 department wanted to give people estimates they'd have to balance: 1) being conservative and overestimating, but people get angry cause it's going to take a long time to get done, 2) underestimating to make people feel optimistic, but then always failing, 3) giving a range and having that range be so big that an estimate is worthless, 4) giving an unbiased point estimate and always being wrong. It may be a huge undertaking, but Group 4 wanted to perform predictions based on not only a real data set but see if the algorithms provided by R can handle very large data sets.  

#### Data Download and Project Preparation

An initial problem with handling a data set with 25+ million rows is finding a repository within the time frame of the project that can meet the needs of the Group. There were two possible options: R-Studio Cloud Pro or Amazon Web Service. R-Studio Cloud Pro costs $25 per month at the student version but it could only handle up to 10 million rows of records. Romerl queried a sales representative from R-Studio Cloud Pro and got a quote of $300 plus a $50 per student cost for usage of such a large data set. which is unacceptable on a student budget for a one+ month project. Ajay found Amazon Web Service and it had four advantages: 1) it was free for one year, 2) it was able to house a table of 25+ million rows and more, 3) the data could be uploaded onto a mySQL database, 4) Group 4 team members could download the data on their own machines with the proper mySQL credentials provided by Ajay.

The next problem facing the Group is the initial downloading of the data. Ajay estimated a download time of 1.5 hours on R to mitigate the 25+ million rows. Ajay decided to split the massive table into 14 sub-tables of approximately 1.8 million rows each. The download time of data even at smaller queries were far more acceptable and allowed the Group to develop the Project further.

Due to Memory Limitations, Machine Limitations, and other factors, the maximum data tested is **520,000 rows of data minus N/A values = 371,670 rows of data total.**

```{r}
# placeholder for maxrows and totalrows
#maxrows = 6400
#totalrows = maxrows * 1 
maxrows = 65000 # max rows that mysql LIMIT can download
totalrows = maxrows * 4 # max rows x8 that R-Studio can process without exceeding 7GB max for one vector
#memory limitation - currently at 520K rows
ntreecand = 420
# Start the clock!
ptm <- proc.time()
cn <- dbConnect(drv      = RMySQL::MySQL(),
                username = "data622",
                password = "group4622",
                host     = "data622.c3alv39zxnhk.us-east-2.rds.amazonaws.com",
                port     = 3306,
                dbname   = "")
mainsqlstr <- paste("SELECT * FROM DATA622.data1;")
#mainsqlstr <- paste("SELECT * FROM DATA622.data1 limit 0,",maxrows,";")
data2 <- dbGetQuery(cn, mainsqlstr)
str(data2)
summary(data2)
dbDisconnect(cn)
```

Database query download time

```{r}
# Stop the clock
proc.time() - ptm
```
We performed some Data Type conversions in order to be able to execute preliminary Data Exploration activities.

```{r}
# Start the clock!
ptm <- proc.time()
data2 <- head(data2,totalrows)
data2$Incident_Zip <- as.numeric(as.character(data2$Incident_Zip))
data2$BBL <- as.numeric(as.character(data2$BBL))
data2$X_Coordinate_State_Plane <- as.numeric(as.character(data2$X_Coordinate_State_Plane))
data2$Y_Coordinate_State_Plane <- as.numeric(as.character(data2$Y_Coordinate_State_Plane))
data2$Latitude <- as.numeric(as.character(data2$Latitude))
data2$Longitude <- as.numeric(as.character(data2$Longitude))
#data2[['Created_Date']] <- strptime(data2[['Created_Date']], format='%Y.%m.%d')
data2$Created_Date <- as.POSIXct(strptime(data2$Created_Date, "%Y-%m-%d %H:%M:%S"))
data2$Closed_Date <- as.POSIXct(strptime(data2$Closed_Date, "%Y-%m-%d %H:%M:%S"))
data2$Resolution_Action_Updated_Date <- as.POSIXct(strptime(data2$Resolution_Action_Updated_Date, "%Y-%m-%d %H:%M:%S"))
data2$Due_Date <- as.POSIXct(strptime(data2$Due_Date, "%Y-%m-%d %H:%M:%S"))

data2$ResolutionTime <- difftime(data2$Closed_Date,data2$Created_Date, units = "min")
data2$ResolutionTime <- as.numeric(data2$ResolutionTime)
# Make it NA if less than 0. Only 75 cases in 1.8 million rows in data1.
data2$ResolutionTime[data2$ResolutionTime < 0] <- NA
```

#### Data Exploration

For our first Data Exploration activity, we did a plot of the Top Complaint Types

```{r}
#Top complaint types
ggplot(subset(data2, data2$Complaint_Type %in% count(data2, data2$Complaint_Type, sort=T)[1:maxrows,]$Complaint_Type), 
       aes(data2$Complaint_Type)) + 
  geom_histogram(stat = "count",color="black", fill="blue") +
  labs(x="Complaint Type", y="Service Requests") +
  coord_flip() + theme_bw()
```

For our second Data Exploration activity, we executed a plot for the Top Borough

```{r}
# Top Borough 
ggplot(subset(data2, data2$Borough %in% count(data2, data2$Borough, sort=T)[1:maxrows,]$Borough), 
       aes(data2$Borough)) + 
  geom_histogram(stat = "count",color="black", fill="blue") +
  labs(x="Borough", y="Service Requests") +
  coord_flip() + theme_bw()
```

For our third Data Exploration activity, we executed a plot of the Top Complaint Types by Borough and Status.

```{r}
# Top complaint types by Borough and Status 
ggplot(subset(data2, data2$Complaint_Type %in% count(data2, Complaint_Type, sort=T)[1:maxrows,]$Complaint_Type), aes(x=data2$Status, y = data2$Complaint_Type)) +
  geom_point() +
  geom_count(n=2, colour="black") + 
  facet_wrap(~data2$Borough)
```

#### Variable Elimination

We will have to get rid of some variables as they may not be needed for model building. For expediency, we will get rid of all NAs instead of imputing them.

- **Unique_Key** - continuous variable - ID - not needed.

- **Agency_Name** - full name of department or organization where 311 calls were made to. Agency is more appropriate.

- **Incident_Address** - full street address - too much disparate information to make an appropriate model

- **Street_Name** - Street name - too much disparate information to make an appropriate model

- **Cross_Street_1** - cross street - too much disparate information to make an appropriate model

- **Cross_Street_1** - cross street - too much disparate information to make an appropriate model

- **Intersection_Street_1** - intersection street - too much disparate information to make an appropriate model

- **Intersection_Street_2** - intersection street - too much disparate information to make an appropriate model

- **City** - Random Forest algorithm cannot handle this variable as it has over 53 categories. The borough of Queens has too many cities that go over the maximum 53 categories.

- **Resolution_Description** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **Landmark** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **X_Coordinate_State_Plane** - x coordinate of approximate location - too much disparate information to make an appropriate model 

- **Y_Coordinate_State_Plane** - y coordinate of approximate location - too much disparate information to make an appropriate model

- **Longitude** - longitude of approximate location - too much disparate information to make an appropriate model

- **Latitude** - latitude of approximate location - too much disparate information to make an appropriate model

- **Location**  - set of latitude and longitude of approximate location - too much disparate information to make an appropriate model

- **Vehicle_Type** - almost all values are NA 

- **Taxi_Company_Borough** - almost all values are NA 

- **Taxi_Pick_Up_Location** - almost all values are NA 

- **Bridge_Highway_Name** - almost all values are NA 

- **Bridge_Highway_Direction** - almost all values are NA 

- **Road_Ramp** - almost all values are NA 

- **Bridge_Highway_Segment** - almost all values are NA

- **Bridge_Highway_Segment** - almost all values are NA

- **Due_Date** - almost all values are NA

- **Facility_Type** - too many values are NA

- **Address_Type** - too many values are NA

- **Park_Borough** - this is the same as Borough

- **Park_Facility_Name** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **Community_Board** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **Descriptor** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **Closed_Date** - Variable is not needed because it is used to calculate ResolutionTime

- **Created_Date** - Variable is not needed because it is used to calculate ResolutionTime

```{r}
data2cand <- data2 %>% 
  select(-Unique_Key, -Agency_Name, -Incident_Address, -Cross_Street_1, -Cross_Street_2, -Intersection_Street_1, -Intersection_Street_2, -City, -Landmark, -Resolution_Description, -Street_Name, -X_Coordinate_State_Plane,  -Y_Coordinate_State_Plane, -Longitude, -Latitude, -Location, -Vehicle_Type, -Taxi_Company_Borough, -Taxi_Pick_Up_Location, -Bridge_Highway_Name, -Bridge_Highway_Direction, -Road_Ramp, -Bridge_Highway_Segment, -Due_Date, -Facility_Type, -Address_Type, -Park_Borough, -Park_Facility_Name, -Community_Board, -Descriptor, -Closed_Date, -Created_Date) %>% 
  mutate_if(is.character, factor)
data2cand <- na.omit(data2cand)

summary(data2cand)
nrow(data2cand)
```

#### Reduce Number of Categorical Variables to Less Than 53

Certain Categorical variables have greater than 53 categories. A Random Forest cannot be executed against these variables because of this. A solution would be to reduce the number of categories by combining like categories together.


**Complaint_Type** Variable

```{r}
data2cand$Complaint_Type <- as.character(data2cand$Complaint_Type)

# Consolidate Noise categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Residential"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Helicopter"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Street/Sidewalk"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Commercial"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Vehicle"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Park"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - House of Worship"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Collection Truck Noise"] <- "Noise"

# Consolidate Sanitary Condition categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "UNSANITARY CONDITION"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Unsanitary Animal Pvt Property"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Indoor Sewage"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Unsanitary Pigeon Condition"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Dirty Conditions"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Root/Sewer/Sidewalk Condition"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Unsanitary Condition"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Wood Pile Remaining"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "DSNY Spillage"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Sweeping/Missed"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Sweeping/Inadequate"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Request Large Bulky Item Collection"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Overflowing Litter Baskets"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Missed Collection (All Materials)"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Litter Basket / Request"] <- "Sanitation Condition"

# Consolidate Tree Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Damaged Tree"] <- "Tree Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Overgrown Tree/Branches"] <- "Tree Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Dead/Dying Tree"] <- "Tree Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Illegal Tree Damage"] <- "Tree Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Uprooted Stump"] <- "Tree Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "New Tree Request"] <- "Tree Issue"

# Consolidate Street Sign Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Street Sign - Damaged"] <- "Street Sign Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Street Sign - Missing"] <- "Street Sign Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Street Sign - Dangling"] <- "Street Sign Issue"

# Consolidate Repair Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "PAINT/PLASTER"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "PLUMBING"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "ELECTRIC"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "APPLIANCE"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "HEAT/HOT WATER"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "WATER LEAK"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "FLOORING/STAIRS"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "DOOR/WINDOW"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "ELEVATOR"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "GENERAL"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "OUTSIDE BUILDING"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Outside Building"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "SAFETY"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "HEATING"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Appliance"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "General"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Safety"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Plumbing"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Peeling Paint"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Mold"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Window Guard"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Non-Residential Heat"] <- "Repair Issue"

# Consolidate Parking Violation categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Blocked Driveway"] <- "Parking Violation"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Illegal Parking"] <- "Parking Violation"

# Consolidate Homeless Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Homeless Encampment"] <- "Homeless Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Homeless Person Assistance"] <- "Homeless Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Homeless Street Condition"] <- "Homeless Issue"

# Consolidate Animal Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Animal-Abuse"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Animal in a Park"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Unleashed Dog"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Rodent"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Mosquitoes"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Illegal Animal Kept as Pet"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Pet Shop"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Animal Facility - No Permit"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Unsanitary Animal Facility"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Illegal Animal Sold"] <- "Animal Issue"

# Consolidate Dining Establishment categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Food Establishment"] <- "Dining Establishment"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Outdoor Dining"] <- "Dining Establishment"

# Consolidate Taxi Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Taxi Compliment"] <- "Taxi Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Taxi Complaint"] <- "Taxi Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "For Hire Vehicle Complaint"] <- "Taxi Issue"

# Consolidate Water Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Drinking Water"] <- "Water Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Bottled Water"] <- "Water Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Standing Water"] <- "Water Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Cooling Tower"] <- "Water Issue"

# Consolidate Bike/Skate-related Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Bike/Roller/Skate Chronic"] <- "Bike/Skate-related Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Derelict Bicycle"] <- "Bike/Skate-related Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Bike Rack Condition"] <- "Bike/Skate-related Issue"

# Consolidate Abandoned Vehicle categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Derelict Vehicles"] <- "Abandoned Vehicle"

# Consolidate Bus Stop Shelter Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Bus Stop Shelter Complaint"] <- "Bus Stop Shelter Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Bus Stop Shelter Placement"] <- "Bus Stop Shelter Issue"

# Consolidate Environmental Issue categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "DEP Street Condition"] <- "Environmental Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "DEP Sidewalk Condition"] <- "Environmental Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Radioactive Material"] <- "Environmental Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Asbestos"] <- "Environmental Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "X-Ray Machine/Equipment"] <- "Environmental Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Indoor Air Quality"] <- "Environmental Issue"

# Consolidate Petty Crimes categories
data2cand$Complaint_Type[data2cand$Complaint_Type == "Urinating in Public"] <- "Petty Crimes"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Squeegee"] <- "Petty Crimes"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Panhandling"] <- "Petty Crimes"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Employee Behavior"] <- "Petty Crimes"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Calorie Labeling"] <- "Petty Crimes"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Illegal Fireworks"] <- "Petty Crimes"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Violation of Park Rules"] <- "Petty Crimes"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Recycling Enforcement"] <- "Petty Crimes"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Disorderly Youth"] <- "Petty Crimes"

data2cand$Complaint_Type <- as.factor(data2cand$Complaint_Type)    
unique(data2cand$Complaint_Type)
```

**Location_Type** Variable

```{r}
# Consolidate Residential categories
data2cand$Location_Type <- as.character(data2cand$Location_Type)
data2cand$Location_Type[data2cand$Location_Type == "RESIDENTIAL BUILDING"] <- "Residential Property"
data2cand$Location_Type[data2cand$Location_Type == "Residential Building/House"] <- "Residential Property"
data2cand$Location_Type[data2cand$Location_Type == "Residential Building"] <- "Residential Property"
data2cand$Location_Type[data2cand$Location_Type == "Residential"] <- "Residential Property"
data2cand$Location_Type[data2cand$Location_Type == "Residence"] <- "Residential Property"
data2cand$Location_Type[data2cand$Location_Type == "Private House"] <- "Residential Property"
data2cand$Location_Type[data2cand$Location_Type == "Private Residence"] <- "Residential Property"

# Consolidate 3+Family Dwelling categories
data2cand$Location_Type[data2cand$Location_Type == "3+ Family Apart"] <- "3+Family Dwelling"
data2cand$Location_Type[data2cand$Location_Type == "3+ Family Apt."] <- "3+Family Dwelling"
data2cand$Location_Type[data2cand$Location_Type == "3+Family Apt."] <- "3+Family Dwelling"
data2cand$Location_Type[data2cand$Location_Type == "3+ Family Apt. Building"] <- "3+Family Dwelling"
data2cand$Location_Type[data2cand$Location_Type == "3+ Family Apartment Building"] <- "3+Family Dwelling"
data2cand$Location_Type[data2cand$Location_Type == "3+ Family Mixed Use Building"] <- "3+Family Dwelling"
data2cand$Location_Type[data2cand$Location_Type == "3+ Family Mixed"] <- "3+Family Dwelling"
data2cand$Location_Type[data2cand$Location_Type == "3+ Family"] <- "3+Family Dwelling"

# Consolidate 1-3 Family Dwelling categories
data2cand$Location_Type[data2cand$Location_Type == "1-3 Family Mixed Use Building"] <- "1-3 Family Dwelling"
data2cand$Location_Type[data2cand$Location_Type == "1-, 2- and 3- Family Home"] <- "1-3 Family Dwelling"

# Consolidate 1-2 Family Dwelling categories
data2cand$Location_Type[data2cand$Location_Type == "1-2 FamilyDwelling"] <- "1-2 Family Dwelling"
data2cand$Location_Type[data2cand$Location_Type == "1-2 Family Mixed Use Building"] <- "1-2 Family Dwelling"

# Consolidate School categories
data2cand$Location_Type[data2cand$Location_Type == "School/Pre-School"] <- "School"
data2cand$Location_Type[data2cand$Location_Type == "Public School"] <- "School"
data2cand$Location_Type[data2cand$Location_Type == "Private School"] <- "School"
data2cand$Location_Type[data2cand$Location_Type == "School - K-12 Private"] <- "School"
data2cand$Location_Type[data2cand$Location_Type == "Cafeteria - Public School"] <- "School"
data2cand$Location_Type[data2cand$Location_Type == "Cafeteria - Private School"] <- "School"
data2cand$Location_Type[data2cand$Location_Type == "School Safety Zone"] <- "School"

# Consolidate Street/Sidewalk categories
data2cand$Location_Type[data2cand$Location_Type == "Sidewalk"] <- "Street/Sidewalk"
data2cand$Location_Type[data2cand$Location_Type == "Street"] <- "Street/Sidewalk"
data2cand$Location_Type[data2cand$Location_Type == "Street/Curbside"] <- "Street/Sidewalk"
data2cand$Location_Type[data2cand$Location_Type == "Street Area"] <- "Street/Sidewalk"
data2cand$Location_Type[data2cand$Location_Type == "Curb"] <- "Street/Sidewalk"

# Consolidate Vacant Property categories
data2cand$Location_Type[data2cand$Location_Type == "Vacant Lot/Property"] <- "Vacant Property"
data2cand$Location_Type[data2cand$Location_Type == "Vacant Lot"] <- "Vacant Property"
data2cand$Location_Type[data2cand$Location_Type == "Vacant Building"] <- "Vacant Property"
data2cand$Location_Type[data2cand$Location_Type == "Abandoned Building"] <- "Vacant Property"

# Consolidate Commercial Property categories
data2cand$Location_Type[data2cand$Location_Type == "Commercial"] <- "Commercial Property"
data2cand$Location_Type[data2cand$Location_Type == "Comercial"] <- "Commercial Property"
data2cand$Location_Type[data2cand$Location_Type == "Commercial Building"] <- "Commercial Property"
data2cand$Location_Type[data2cand$Location_Type == "Store/Commercial"] <- "Commercial Property"

# Consolidate Parking Lot/Garage categories
data2cand$Location_Type[data2cand$Location_Type == "Parking Lot"] <- "Parking Lot/Garage"

# Consolidate Park/Garden categories
data2cand$Location_Type[data2cand$Location_Type == "Public Park/Garden"] <- "Park/Garden"
data2cand$Location_Type[data2cand$Location_Type == "Public Garden/Park"] <- "Park/Garden"
data2cand$Location_Type[data2cand$Location_Type == "Public Garden"] <- "Park/Garden"
data2cand$Location_Type[data2cand$Location_Type == "Park"] <- "Park/Garden"

# Consolidate Other categories
data2cand$Location_Type[data2cand$Location_Type == "N/A"] <- "Other"
data2cand$Location_Type[data2cand$Location_Type == "Other (Explain Below)"] <- "Other"
data2cand$Location_Type[data2cand$Location_Type == "Other (explain in Complaint Details)"] <- "Other"
data2cand$Location_Type[data2cand$Location_Type == "Property Address"] <- "Other"
data2cand$Location_Type[data2cand$Location_Type == "Street Address"] <- "Other"
data2cand$Location_Type[data2cand$Location_Type == "Above Address"] <- "Other"
data2cand$Location_Type[data2cand$Location_Type == "Mixed Use"] <- "Other"

# Consolidate Pool/Spa/Beach categories
data2cand$Location_Type[data2cand$Location_Type == "Pool"] <- "Pool/Spa/Beach"
data2cand$Location_Type[data2cand$Location_Type == "Beach"] <- "Pool/Spa/Beach"
data2cand$Location_Type[data2cand$Location_Type == "Sauna"] <- "Pool/Spa/Beach"
data2cand$Location_Type[data2cand$Location_Type == "Spa Pool"] <- "Pool/Spa/Beach"

# Consolidate Medical/Dental Practice categories
data2cand$Location_Type[data2cand$Location_Type == "Doctor's Office"] <- "Medical/Dental Practice"
data2cand$Location_Type[data2cand$Location_Type == "Dentist's Office"] <- "Medical/Dental Practice"

# Consolidate Access Area categories
data2cand$Location_Type[data2cand$Location_Type == "Lobby"] <- "Access Area"
data2cand$Location_Type[data2cand$Location_Type == "Common Area"] <- "Access Area"
data2cand$Location_Type[data2cand$Location_Type == "Hallway"] <- "Access Area"
data2cand$Location_Type[data2cand$Location_Type == "Stairwell"] <- "Access Area"
data2cand$Location_Type[data2cand$Location_Type == "Public/Unfenced Area"] <- "Access Area"
data2cand$Location_Type[data2cand$Location_Type == "Public Stairs"] <- "Access Area"
data2cand$Location_Type[data2cand$Location_Type == "Public Plaza"] <- "Access Area"

# Consolidate Street Vendor categories
data2cand$Location_Type[data2cand$Location_Type == "Food Cart Vendor"] <- "Street Vendor"
data2cand$Location_Type[data2cand$Location_Type == "Street Fair Vendor"] <- "Street Vendor"

# Consolidate Store categories
data2cand$Location_Type[data2cand$Location_Type == "House and Store"] <- "Store"
data2cand$Location_Type[data2cand$Location_Type == "Retail Store"] <- "Store"
data2cand$Location_Type[data2cand$Location_Type == "Grocery Store"] <- "Store"

# Consolidate Building categories
data2cand$Location_Type[data2cand$Location_Type == "Building (Non-Residential)"] <- "Building"
data2cand$Location_Type[data2cand$Location_Type == "Government Building"] <- "Building"
data2cand$Location_Type[data2cand$Location_Type == "Office Building"] <- "Building"

# Consolidate Animal-based Business categories
data2cand$Location_Type[data2cand$Location_Type == "Horse Stable"] <- "Animal-based Business"
data2cand$Location_Type[data2cand$Location_Type == "Petting Zoo/Animal Exhibit"] <- "Animal-based Business"
data2cand$Location_Type[data2cand$Location_Type == "Kennel/Animal Shelter"] <- "Animal-based Business"
data2cand$Location_Type[data2cand$Location_Type == "Groomer"] <- "Animal-based Business"

# Consolidate Restaurant/Bar/Deli/Bakery categories
data2cand$Location_Type[data2cand$Location_Type == "Restaurant"] <- "Restaurant/Bar/Deli/Bakery"
data2cand$Location_Type[data2cand$Location_Type == "Club/Bar/Restaurant"] <- "Restaurant/Bar/Deli/Bakery"

# Consolidate 'One-Hit Wonder' categories
data2cand$Location_Type[data2cand$Location_Type == "Subway Station"] <- "Subway"

data2cand$Location_Type <- as.factor(data2cand$Location_Type)    
unique(data2cand$Location_Type)
```

#### Create Variable Acceptable_Resolution_Status

Upon executing the summary for ResolutionTime, we have a very wide set of values for ResolutionTime. At 520,000 rows, the mean for ResolutionTime is an unacceptabe 9772.2 minutes while the median is an unacceptable 92.3 minutes. We will create a variable called Acceptable_Resolution_Status with 'Y' for any value between 1 and 92.3 minutes and 'N' for 0 value or any value greater than 92.23 minutes.

Note: The median and mean for the 1.8 million data set are 196.3 minutes and 7368.5 minutes respectively. For 65000 records, the median and mean are 32.4 minutes and 7368.5 minutes respectively. 
```{r}
medianARS <- 92.3
data2cand$Acceptable_Resolution_Status = case_when(data2cand$ResolutionTime == 0 ~ 'N', 
                        (data2cand$ResolutionTime > 0 & data2cand$ResolutionTime <= medianARS) ~ 'Y', 
                        data2cand$ResolutionTime > medianARS ~ 'N')
summary(data2cand$ResolutionTime)
data2cand$Acceptable_Resolution_Status <- as.factor(data2cand$Acceptable_Resolution_Status)
summary(data2cand)
```

#### Split Data into Training and Test Data Sets

Next we will split the data into training and test data sets

```{r}
set.seed(100)
inTrain1 <- createDataPartition(y = data2cand$Acceptable_Resolution_Status, p=0.75, list = FALSE)
training1 <- data2cand[inTrain1,]
testing1 <- data2cand[-inTrain1,]
```

## (2) Random Forest {.tabset .tabset-fade .tabset-pills} 

The random forest algorithm works well with all sorts of data:
numeric and categorical, un-scaled and scaled, full rank and highly correlative.

#### Model 1 - Default (which has factor variables in single columns)

```{r}
set.seed(521)
m1 <- randomForest(Acceptable_Resolution_Status ~ .- ResolutionTime, data = training1)
print(m1)
plot(m1)
importance(m1)
varImpPlot(m1)
```

We ran the random forest for the default model and we were very satisfied with the results. There is an error rate of 0%. We will find out later when we run the confusion matrix of the prediction values that Model 1 has an accuracy of 100%, a Kappa of 100%, and a Sensitivity of 100%. Moreover, a normal plot for a random forest should show a curve sloping down near the X and Y axes. There are 3 different curves almost parellel to each other. You will observe in the Variable Importance Plot and Summary that ResolutionTime has an extremely high Coeefficient. While it is a major predictor for the Application_Resolution_Status, it is highly correlated as we used the values of ResolutionTime to create the dependent variable Application_Resolution_Status. The next 3 most important variables afterwards - Resolution_Action_Updated_Date, Agency, and Complaint_Type - may show promise because they are variables from the original data set. We decided to keep the **m1** model as Model 1 for comparisons with the other Random Forest models in this section. 


#### Model 2 - Default Model minus ResolutionTime variable with tuning for mtry

Prior to running Model 3, we ran tuneRF function to find out what is the optimal **mtry** variable that will have the lowest error rate and hopefully increase accuracy.

```{r}
# names of features
training1a <- training1 %>% 
  select(-ResolutionTime)
features <- setdiff(names(training1a), "Acceptable_Resolution_Status")

set.seed(123)

m3a <- tuneRF(
  x          = training1a[features],
  y          = training1a$Acceptable_Resolution_Status,
  ntreeTry   = 500,
  mtryStart  = 5,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)
best.m <- m3a[m3a[, 2] == min(m3a[, 2]), 1]
print(m3a)
print(best.m)
bestmtry <- 9
```

It was determined that the optiomal **mtry** parameter is 9. We reran the Random Forest to see if the accuracy improved.

```{r}
set.seed(521)
m2 <- randomForest(Acceptable_Resolution_Status ~ . - ResolutionTime, data = training1, mtry = bestmtry)
print(m2)
plot(m2)
importance(m2)
varImpPlot(m2)
```

#### Model 3 - Default Model minus ResolutionTime variable with tuning for mtry=9 and ntree

```{r}
set.seed(521)
m3 <- randomForest(Acceptable_Resolution_Status ~ . - ResolutionTime, data = training1, mtry = bestmtry, ntree = ntreecand)
print(m3)
plot(m3)
importance(m3)
varImpPlot(m3)
```

We ran the random forest for the default model and we were satisfied with the results. There is an error rate of 16.72%. We will find out later when we run the confusion matrix of the prediction values that Model 2 has an accuracy of 84.45%, a Kappa of 62.98%, and a Sensitivity of 86.35%. Moreover, a normal plot for a random forest should show a curve sloping down near the X and Y axes. There are 3 different curves almost parellel to each other. You will observe in the Variable Importance Plot and Summary that the 3 most important variables are Resolution_Action_Updated_Date, Agency, and Complaint_Type . BBL is not too far behind. We decided to keep the **m2** model as Model 2 for comparisons with the other Random Forest models in this section. 

Random Forest processing time
```{r}
# Stop the clock
proc.time() - ptm
```
We ran the random forest for the default model and we were satisfied with the results. There is an error rate of 15.12%. We will find out later when we run the confusion matrix of the prediction values that Model 2 has an accuracy of 85.13%, a Kappa of 66.54%, and a Sensitivity of 88.44%. Moreover, a normal plot for a random forest should show a curve sloping down near the X and Y axes. There are 3 different curves almost parellel to each other. You will observe in the Variable Importance Plot and Summary that the 3 most important variables are Resolution_Action_Updated_Date, Agency, and Borough-Block-Lot (BBL). We decided to keep the **m3** model as Model 3 for comparisons with the other Random Forest models in this section. 

## (3) Support Vector Machine {.tabset .tabset-fade .tabset-pills} 

Support Vector Machines (SVM's) seem like a good model to investigate because it can deal with high-dimensional data. The data we have not only several variables, but it also has categorical variables with several levels for each. We will see if SVM can provide a suitable model to deal with this issue. 

#### Remove ResolutionTime

As indicated in the Random Forest exercise, we should remove the ResolutionTime variable as it heavily influences Application_Resolution_Status.

```{r}
# Start the clock!
ptm <- proc.time()
# names of features
data2canda <- data2cand %>% 
  select(-ResolutionTime)
```

#### Split Data into Training and Test Data Sets

Next we will split the data into training and test data sets

```{r}
set.seed(100)
roww <- nrow(data2canda)
numTrain <- floor(0.75 * roww)
numTest <- roww - numTrain
training2 <- data2canda[sample(roww, numTrain), ]
testing2 <- data2canda[sample(roww, numTest), ]
```

#### One-Hot Encoding of Categorical Variables

SVM's do not like categorical variables. We will have to use one hot-encoding to make dummy variables with numerical replacements.

```{r}
dummies <- dummyVars(~ ., data=training2[,-10])
c2 <- predict(dummies, training2[,-10])
d_training <- as.data.frame(cbind(training2$Acceptable_Resolution_Status,c2))
str(d_training)

dummies <- dummyVars(~ ., data=testing2[,-10])
c2 <- predict(dummies, testing2[,-10])
d_testing <- as.data.frame(cbind(testing2$Acceptable_Resolution_Status,c2))
str(d_testing)
```
#### SVM Model 1 Default with Tunning Cost 1-10 using Linear Kernel

Optimal Cost 9 at 400 rows - using SVM tuning - 16 minutes
Optimal Cost at 6 at 1600 rows - using SVM tuning = 56 minutes
```{r}
#gammalist <- c(0.005,0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05)
#tune.out <- tune.svm(as.factor(V1) ~., data=d_training, 
#                 kernel='linear', cost=1:10, gamma = gammalist)
#summary(tune.out)
#svm1 <- tune.out$best.model
svm1 <- svm(as.factor(V1) ~., data=d_training, kernel='linear', cost=6, gamma = 0.005)
summary(svm1)
```

#### SVM Model 2 Execution with Tuning Gamma Range 0.005-0.05 and Cost 2^(-1:5) using Radial Kernel

Optimal Gamma 0.005 and Cost 1 at 400 rows

```{r}
#gammalist <- c(0.005,0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05)
#tune.out <- tune.svm(as.factor(V1) ~., data=d_training, 
#                 kernel='radial', cost=2^(-1:5), gamma = gammalist)
#summary(tune.out)
#svm2 <- tune.out$best.model
svm2 <- svm(as.factor(V1) ~., data=d_training, kernel='radial', cost=1, gamma = 0.005)
summary(svm2)
```

#### SVM Model 3 Execution with Cost 1-5 using Polynomial Kernel

Model is too expensive to run even at 400 rows.

```{r}
#svm3 <- svm(V1 ~., data=d_training, kernel='polynomial', cost=1, gamma = 0.005)
#summary(svm3)
```

Support Vector Machine processing time
```{r}
# Stop the clock
proc.time() - ptm
```

## (4) Decision Tree {.tabset .tabset-fade .tabset-pills} 

Decision trees are a supervised machine learning method used for classification and regression problems. This method operates by segmentation and aims to reduce the entropy of a dataset by continually splitting the data in a tree like structure by the most informative variables

#### Remove ResolutionTime

As indicated in the other models, we should remove the ResolutionTime variable as it heavily influences Application_Resolution_Status.

```{r}
# Start the clock!
ptm <- proc.time()
# names of features
data2canda <- data2cand %>% 
  select(-ResolutionTime)
```

#### Split Data into Training and Test Data Sets and Dummifying

Next we will use the split from the SVM processes in our model. and we will use one hot-encoding to make dummy variables with numerical replacements. We will use the same export from the SVM dataset


#### Decision Tree Model

Optimal Cost 9 at 400 rows - using SVM tuning - 16 minutes
Optimal Cost at 6 at 1600 rows - using SVM tuning = 56 minutes
```{r}
#gammalist <- c(0.005,0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05)
#tune.out <- tune.svm(as.factor(V1) ~., data=d_training, 
#                 kernel='linear', cost=1:10, gamma = gammalist)
#summary(tune.out)
#svm1 <- tune.out$best.model

dummies <- dummyVars(~ ., data=training2[,-10])
c2 <- predict(dummies, training2[,-10]) %>% as.data.frame() %>% dplyr::select(-contains("Acceptable"))
d_training2 <- as.data.frame(bind_cols(training2$Acceptable_Resolution_Status,c2)) %>% 
  rename("Response"="...1")
str(d_training2)

dummies <- dummyVars(~ ., data=testing2[,-10])
c2 <- predict(dummies, testing2[,-10])%>% as.data.frame() %>% dplyr::select(-contains("Acceptable"))
d_testing2 <- as.data.frame(bind_cols(testing2$Acceptable_Resolution_Status,c2)) %>% 
  rename("Response"="...1")
str(d_testing2)

dt1 <-rpart::rpart(Response ~., data=d_training2)
summary(dt1)
varImp(dt1) %>% as.data.frame() %>% 
  rownames_to_column() %>% 
  slice_max(Overall, n = 5) %>% 
  ggplot(aes(x = fct_reorder(rowname,Overall), y = Overall))+
    geom_col(fill = "skyblue")+
  coord_flip()+
  labs(title = "Variable importance for Decision Tree",
       x = "Score",
       y = "Feature")
```

## (5) Model Performance and Selection {.tabset .tabset-fade .tabset-pills}

#### Random Forest

```{r}
# Start the clock!
ptm <- proc.time()
rfpred1 <- predict(m1,newdata = testing1)
rfpred1.cm <- confusionMatrix(rfpred1, testing1$Acceptable_Resolution_Status) 
rfpred1.cm
```

```{r}
rfpred2 <- predict(m2,newdata = testing1)
rfpred2.cm <- confusionMatrix(rfpred2, testing1$Acceptable_Resolution_Status) 
rfpred2.cm
```

```{r}
rfpred3 <- predict(m3,newdata = testing1)
rfpred3.cm <- confusionMatrix(rfpred3, testing1$Acceptable_Resolution_Status) 
rfpred3.cm
```

#### Support Vector Machine Predictions

```{r}
svmpred1 <- predict(svm1, d_testing[,-1])
svmpred1.cm <- confusionMatrix(svmpred1, as.factor(d_testing$V1))
```

```{r}
svmpred2 <- predict(svm2, d_testing[,-1])
svmpred2.cm <- confusionMatrix(svmpred2, as.factor(d_testing$V1))
```

```{r}
```

#### Decision Tree Predictions

```{r}
dtpred1 <- predict(dt1, d_testing2[,-1], type = "class")
dtpred1.cm <- confusionMatrix(dtpred1, d_testing2$Response)
```

```{r}
# RF Model 1 Values
rfpred1.accuracy <- rfpred1.cm$overall['Accuracy']
rfpred1.kappa <- rfpred1.cm$overall['Kappa']
rfpred1.sensitivity <- rfpred1.cm$byClass['Sensitivity']
rfpred1.TN <- rfpred1.cm$table[1,1]
rfpred1.FP <- rfpred1.cm$table[1,2]
rfpred1.FN <- rfpred1.cm$table[2,1]
rfpred1.TP <- rfpred1.cm$table[2,2]
rfpred1.TPR <- rfpred1.TP /(rfpred1.TP + rfpred1.FN)
rfpred1.TNR <- rfpred1.TN /(rfpred1.TN + rfpred1.FP)
rfpred1.FPR <- rfpred1.FP /(rfpred1.TN + rfpred1.FP)
rfpred1.FNR <- rfpred1.FN /(rfpred1.TP + rfpred1.FN)
rfpred1.precision <- rfpred1.TP / (rfpred1.TP + rfpred1.FP)
rfpred1.recall <- rfpred1.TP / (rfpred1.TP + rfpred1.FN)
rfpred1.specificity <- rfpred1.TN / (rfpred1.TN + rfpred1.FP)
rfpred1.f1score <- 2 * ((rfpred1.precision * rfpred1.recall) / (rfpred1.precision + rfpred1.recall))
```

```{r}
# RF Model 2 Values
rfpred2.accuracy <- rfpred2.cm$overall['Accuracy']
rfpred2.kappa <- rfpred2.cm$overall['Kappa']
rfpred2.sensitivity <- rfpred2.cm$byClass['Sensitivity']
rfpred2.TN <- rfpred2.cm$table[1,1]
rfpred2.FP <- rfpred2.cm$table[1,2]
rfpred2.FN <- rfpred2.cm$table[2,1]
rfpred2.TP <- rfpred2.cm$table[2,2]
rfpred2.TPR <- rfpred2.TP /(rfpred2.TP + rfpred2.FN)
rfpred2.TNR <- rfpred2.TN /(rfpred2.TN + rfpred2.FP)
rfpred2.FPR <- rfpred2.FP /(rfpred2.TN + rfpred2.FP)
rfpred2.FNR <- rfpred2.FN /(rfpred2.TP + rfpred2.FN)
rfpred2.precision <- rfpred2.TP / (rfpred2.TP + rfpred2.FP)
rfpred2.recall <- rfpred2.TP / (rfpred2.TP + rfpred2.FN)
rfpred2.specificity <- rfpred2.TN / (rfpred2.TN + rfpred2.FP)
rfpred2.f1score <- 2 * ((rfpred2.precision * rfpred2.recall) / (rfpred2.precision + rfpred2.recall))
```

```{r}
# RF Model 3 Values
rfpred3.accuracy <- rfpred3.cm$overall['Accuracy']
rfpred3.kappa <- rfpred3.cm$overall['Kappa']
rfpred3.sensitivity <- rfpred3.cm$byClass['Sensitivity']
rfpred3.TN <- rfpred3.cm$table[1,1]
rfpred3.FP <- rfpred3.cm$table[1,2]
rfpred3.FN <- rfpred3.cm$table[2,1]
rfpred3.TP <- rfpred3.cm$table[2,2]
rfpred3.TPR <- rfpred3.TP /(rfpred3.TP + rfpred3.FN)
rfpred3.TNR <- rfpred3.TN /(rfpred3.TN + rfpred3.FP)
rfpred3.FPR <- rfpred3.FP /(rfpred3.TN + rfpred3.FP)
rfpred3.FNR <- rfpred3.FN /(rfpred3.TP + rfpred3.FN)
rfpred3.precision <- rfpred3.TP / (rfpred3.TP + rfpred3.FP)
rfpred3.recall <- rfpred3.TP / (rfpred3.TP + rfpred3.FN)
rfpred3.specificity <- rfpred3.TN / (rfpred3.TN + rfpred3.FP)
rfpred3.f1score <- 2 * ((rfpred3.precision * rfpred3.recall) / (rfpred3.precision + rfpred3.recall))
```

```{r}
# RF Model 1 Values
svmpred1.accuracy <- svmpred1.cm$overall['Accuracy']
svmpred1.kappa <- svmpred1.cm$overall['Kappa']
svmpred1.sensitivity <- svmpred1.cm$byClass['Sensitivity']
svmpred1.TN <- svmpred1.cm$table[1,1]
svmpred1.FP <- svmpred1.cm$table[1,2]
svmpred1.FN <- svmpred1.cm$table[2,1]
svmpred1.TP <- svmpred1.cm$table[2,2]
svmpred1.TPR <- svmpred1.TP /(svmpred1.TP + svmpred1.FN)
svmpred1.TNR <- svmpred1.TN /(svmpred1.TN + svmpred1.FP)
svmpred1.FPR <- svmpred1.FP /(svmpred1.TN + svmpred1.FP)
svmpred1.FNR <- svmpred1.FN /(svmpred1.TP + svmpred1.FN)
svmpred1.precision <- svmpred1.TP / (svmpred1.TP + svmpred1.FP)
svmpred1.recall <- svmpred1.TP / (svmpred1.TP + svmpred1.FN)
svmpred1.specificity <- svmpred1.TN / (svmpred1.TN + svmpred1.FP)
svmpred1.f1score <- 2 * ((svmpred1.precision * svmpred1.recall) / (svmpred1.precision + svmpred1.recall))
```

```{r}
# RF Model 2 Values
svmpred2.accuracy <- svmpred2.cm$overall['Accuracy']
svmpred2.kappa <- svmpred2.cm$overall['Kappa']
svmpred2.sensitivity <- svmpred2.cm$byClass['Sensitivity']
svmpred2.TN <- svmpred2.cm$table[1,1]
svmpred2.FP <- svmpred2.cm$table[1,2]
svmpred2.FN <- svmpred2.cm$table[2,1]
svmpred2.TP <- svmpred2.cm$table[2,2]
svmpred2.TPR <- svmpred2.TP /(svmpred2.TP + svmpred2.FN)
svmpred2.TNR <- svmpred2.TN /(svmpred2.TN + svmpred2.FP)
svmpred2.FPR <- svmpred2.FP /(svmpred2.TN + svmpred2.FP)
svmpred2.FNR <- svmpred2.FN /(svmpred2.TP + svmpred2.FN)
svmpred2.precision <- svmpred2.TP / (svmpred2.TP + svmpred2.FP)
svmpred2.recall <- svmpred2.TP / (svmpred2.TP + svmpred2.FN)
svmpred2.specificity <- svmpred2.TN / (svmpred2.TN + svmpred2.FP)
svmpred2.f1score <- 2 * ((svmpred2.precision * svmpred2.recall) / (svmpred2.precision + svmpred2.recall))
```

```{r}
# RF Model 1 Values
dtpred1.accuracy <- dtpred1.cm$overall['Accuracy']
dtpred1.kappa <- dtpred1.cm$overall['Kappa']
dtpred1.sensitivity <- dtpred1.cm$byClass['Sensitivity']
dtpred1.TN <- dtpred1.cm$table[1,1]
dtpred1.FP <- dtpred1.cm$table[1,2]
dtpred1.FN <- dtpred1.cm$table[2,1]
dtpred1.TP <- dtpred1.cm$table[2,2]
dtpred1.TPR <- dtpred1.TP /(dtpred1.TP + dtpred1.FN)
dtpred1.TNR <- dtpred1.TN /(dtpred1.TN + dtpred1.FP)
dtpred1.FPR <- dtpred1.FP /(dtpred1.TN + dtpred1.FP)
dtpred1.FNR <- dtpred1.FN /(dtpred1.TP + dtpred1.FN)
dtpred1.precision <- dtpred1.TP / (dtpred1.TP + dtpred1.FP)
dtpred1.recall <- dtpred1.TP / (dtpred1.TP + dtpred1.FN)
dtpred1.specificity <- dtpred1.TN / (dtpred1.TN + dtpred1.FP)
dtpred1.f1score <- 2 * ((dtpred1.precision * dtpred1.recall) / (dtpred1.precision + dtpred1.recall))
```


```{r, echo=FALSE}
Model <- c("RF 1","RF 2", "RF 3", "SVM 1", "SVM 2", "DT 1")
Accuracy <- c(rfpred1.accuracy, rfpred2.accuracy, rfpred3.accuracy, svmpred1.accuracy, svmpred2.accuracy, dtpred1.accuracy)
Kappa <- c(rfpred1.kappa, rfpred2.kappa, rfpred3.kappa, svmpred1.kappa, svmpred2.kappa, dtpred1.kappa)
Sensitivity <- c(rfpred1.sensitivity, rfpred2.sensitivity, rfpred3.sensitivity, svmpred1.sensitivity, svmpred2.sensitivity, dtpred1.sensitivity)
Recall <- c(rfpred1.recall, rfpred2.recall, rfpred3.recall,svmpred1.recall, svmpred2.recall, dtpred1.recall)
Specificity <- c(rfpred1.specificity, rfpred2.specificity, rfpred3.specificity, svmpred1.specificity, svmpred2.specificity, dtpred1.sensitivity)
Precision <- c(rfpred1.precision, rfpred2.precision, rfpred3.precision, svmpred1.precision, svmpred2.precision,dtpred1.precision)
F1Score <- c(rfpred1.f1score, rfpred2.f1score, rfpred3.f1score, svmpred1.f1score, svmpred2.f1score,dtpred1.f1score)
TPR <- c(rfpred1.TPR, rfpred2.TPR, rfpred3.TPR, svmpred1.TPR, svmpred2.TPR, dtpred1.TPR)
TNR <- c(rfpred1.TNR, rfpred2.TNR, rfpred3.TNR, svmpred1.TNR, svmpred2.TNR, dtpred1.TNR)
FPR <- c(rfpred1.FPR, rfpred2.FPR, rfpred3.FPR, svmpred1.FPR, svmpred2.FPR, dtpred1.FPR)
FNR <- c(rfpred1.FNR, rfpred2.FNR, rfpred3.FNR, svmpred1.FNR, svmpred2.FNR, dtpred1.FNR)
tableModel <- data.frame(Model,Accuracy,Kappa, Sensitivity, Recall,Specificity,Precision,F1Score)
tableModel %>%
  kable() %>%
  kable_styling()

RF2String <- paste("RF 2 and  3 mtry = ",bestmtry,"\n")
print(RF2String)
RF3String <- paste("RF 2 ntree = 500, RF 3 ntree = ",ntreecand,"\n")
print(RF3String)
```

Prediction and Confusion Matrix processing time
```{r}
# Stop the clock
proc.time() - ptm
```
