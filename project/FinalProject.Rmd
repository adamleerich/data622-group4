---
title: "Group 4 Final Project"
author: "Ajay Arora, Romerl Elizes, Jimmy Ng, Joshua Registe, Adam Rich"
date: "May 23, 2021"
output:
  rmdformats::readthedown:
    self_contained: yes
    thumbnails: yes
    lightbox: yes
    gallery: no
    highlight: tango
    code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE,
  cache = FALSE)
packages <- c(
  'tidyverse', 
  'corrplot', 
  'palmerpenguins',
  'class',
  'kableExtra',
  'naniar',
  'mice',
  'DataExplorer',
  'caret',
  'tidymodels',
  'rsample',
  'themis',
  'randomForest',
  'car',
  'xgboost',
  'broom',
  'DBI',
  'RMySQL',
  'RJDBC'
)
for (pkg in packages) {
  suppressPackageStartupMessages(suppressWarnings(
    library(
      pkg, character.only = TRUE, 
      warn.conflicts = FALSE, quietly = TRUE)
  ))
}
# A ggplot2 function
defaulttheme <- theme(
  panel.background = element_blank(),
  panel.border = element_rect(color = "black", fill = NA))
```
 
## (1) Data Prep and Mitigation {.tabset .tabset-fade .tabset-pills} 

#### Introduction

After brainstorming the data sets available, Group 4 decided to use the New York City (NYC) 311 data found in the link: https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9. It is a very ambitious undertaking given that all Assignments covered in this course consisted of no more than 614 rows of records. The NYC data consists of 25+ million rows and 41 columns.
The initial goal of the project is to develop a prediction algorithm from 3 possible models that would allow 311 operators to give an estimate of when the request will be resolved?  It's a simple statement, but a complicated problem.  If the 311 department wanted to give people estimates they'd have to balance: 1) being conservative and overestimating, but people get angry cause it's going to take a long time to get done, 2) underestimating to make people feel optimistic, but then always failing, 3) giving a range and having that range be so big that an estimate is worthless, 4) giving an unbiased point estimate and always being wrong. It may be a huge undertaking, but Group 4 wanted to perform predictions based on not only a real data set but see if the algorithms provided by R can handle very large data sets.  

#### Data Download and Project Preparation

An initial problem with handling a data set with 25+ million rows is finding a repository within the time frame of the project that can meet the needs of the Group. There were two possible options: R-Studio Cloud Pro or Amazon Web Service. R-Studio Cloud Pro costs $25 per month at the student version but it could only handle up to 10 million rows of records. Romerl queried a sales representative from R-Studio Cloud Pro and got a quote of $300 plus a $50 per student cost for usage of such a large data set. which is unacceptable on a student budget for a one+ month project. Ajay found Amazon Web Service and it had four advantages: 1) it was free for one year, 2) it was able to house a table of 25+ million rows and more, 3) the data could be uploaded onto a mySQL database, 4) Group 4 team members could download the data on their own machines with the proper mySQL credentials provided by Ajay.

The next problem facing the Group is the initial downloading of the data. Ajay estimated a download time of 1.5 hours on R to mitigate the 25+ million rows. Ajay decided to split the massive table into 14 sub-tables of approximately 1.8 million rows each. The download time of data even at smaller queries were far more acceptable and allowed the Group to develop the Project further.

```{r}
# placeholder for maxrows
maxrows = 65000
# Start the clock!
ptm <- proc.time()
cn <- dbConnect(drv      = RMySQL::MySQL(),
                username = "data622",
                password = "group4622",
                host     = "data622.c3alv39zxnhk.us-east-2.rds.amazonaws.com",
                port     = 3306,
                dbname   = "")
# I queried column names from ny311 so we can decide which variables to eliminate and use a for-loop to concatenate a string for the select statement we will use to download data.
# - WE MAY NOT NEED THIS.
#colsqlstr <- "SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE table_name = 'data1';"
#colnms <- dbGetQuery(cn, colsqlstr)
#colnmslist <- list(colnms$COLUMN_NAME)
#str(colnmslist)
#mainsqlstr <- paste("SELECT * FROM DATA622.data1 order by rand() limit 0,",maxrows,";")
mainsqlstr <- paste("SELECT * FROM DATA622.data1 limit 0,",maxrows,";")
data2 <- dbGetQuery(cn, mainsqlstr)
str(data2)
summary(data2)
dbDisconnect(cn)
```

Database query download time

```{r}
# Stop the clock
proc.time() - ptm
```
We performed some Data Type conversions in order to be able to execute preliminary Data Exploration activities.

```{r}
# Start the clock!
ptm <- proc.time()
data2$Incident_Zip <- as.numeric(as.character(data2$Incident_Zip))
data2$BBL <- as.numeric(as.character(data2$BBL))
data2$X_Coordinate_State_Plane <- as.numeric(as.character(data2$X_Coordinate_State_Plane))
data2$Y_Coordinate_State_Plane <- as.numeric(as.character(data2$Y_Coordinate_State_Plane))
data2$Latitude <- as.numeric(as.character(data2$Latitude))
data2$Longitude <- as.numeric(as.character(data2$Longitude))
#data2[['Created_Date']] <- strptime(data2[['Created_Date']], format='%Y.%m.%d')
data2$Created_Date <- as.POSIXct(strptime(data2$Created_Date, "%Y-%m-%d %H:%M:%S"))
data2$Closed_Date <- as.POSIXct(strptime(data2$Closed_Date, "%Y-%m-%d %H:%M:%S"))
data2$Resolution_Action_Updated_Date <- as.POSIXct(strptime(data2$Resolution_Action_Updated_Date, "%Y-%m-%d %H:%M:%S"))
data2$Due_Date <- as.POSIXct(strptime(data2$Due_Date, "%Y-%m-%d %H:%M:%S"))

data2$ResolutionTime <- difftime(data2$Closed_Date,data2$Created_Date, units = "min")
data2$ResolutionTime <- as.numeric(data2$ResolutionTime)
```

#### Data Exploration

For our first Data Exploration activity, we did a plot of the Top Complaint Types

```{r}
#Top complaint types
ggplot(subset(data2, data2$Complaint_Type %in% count(data2, data2$Complaint_Type, sort=T)[1:maxrows,]$Complaint_Type), 
       aes(data2$Complaint_Type)) + 
  geom_histogram(stat = "count",color="black", fill="blue") +
  labs(x="Complaint Type", y="Service Requests") +
  coord_flip() + theme_bw()
```

For our second Data Exploration activity, we executed a plot for the Top Borough

```{r}
# Top Borough 
ggplot(subset(data2, data2$Borough %in% count(data2, data2$Borough, sort=T)[1:maxrows,]$Borough), 
       aes(data2$Borough)) + 
  geom_histogram(stat = "count",color="black", fill="blue") +
  labs(x="Borough", y="Service Requests") +
  coord_flip() + theme_bw()
```

For our third Data Exploration activity, we executed a plot of the Top Complaint Types by Borough and Status.

```{r}
# Top complaint types by Borough and Status 
ggplot(subset(data2, data2$Complaint_Type %in% count(data2, Complaint_Type, sort=T)[1:maxrows,]$Complaint_Type), aes(x=data2$Status, y = data2$Complaint_Type)) +
  geom_point() +
  geom_count(n=2, colour="black") + 
  facet_wrap(~data2$Borough)
```

#### Variable Elimination

We will have to get rid of some variables as they may not be needed for model building. For expediency, we will get rid of all NAs instead of imputing them.

- **Unique_Key** - continuous variable - ID - not needed.

- **Agency_Name** - full name of department or organization where 311 calls were made to. Agency is more appropriate.

- **Incident_Address** - full street address - too much disparate information to make an appropriate model

- **Street_Name** - Street name - too much disparate information to make an appropriate model

- **Cross_Street_1** - cross street - too much disparate information to make an appropriate model

- **Cross_Street_1** - cross street - too much disparate information to make an appropriate model

- **Intersection_Street_1** - intersection street - too much disparate information to make an appropriate model

- **Intersection_Street_2** - intersection street - too much disparate information to make an appropriate model

- **City** - Random Forest algorithm cannot handle this variable as it has over 53 categories. The borough of Queens has too many cities that go over the maximum 53 categories.

- **Resolution_Description** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **Landmark** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **X_Coordinate_State_Plane** - x coordinate of approximate location - too much disparate information to make an appropriate model 

- **Y_Coordinate_State_Plane** - y coordinate of approximate location - too much disparate information to make an appropriate model

- **Longitude** - longitude of approximate location - too much disparate information to make an appropriate model

- **Latitude** - latitude of approximate location - too much disparate information to make an appropriate model

- **Location**  - set of latitude and longitude of approximate location - too much disparate information to make an appropriate model

- **Vehicle_Type** - almost all values are NA 

- **Taxi_Company_Borough** - almost all values are NA 

- **Taxi_Pick_Up_Location** - almost all values are NA 

- **Bridge_Highway_Name** - almost all values are NA 

- **Bridge_Highway_Direction** - almost all values are NA 

- **Road_Ramp** - almost all values are NA 

- **Bridge_Highway_Segment** - almost all values are NA

- **Bridge_Highway_Segment** - almost all values are NA

- **Due_Date** - almost all values are NA

- **Facility_Type** - too many values are NA

- **Address_Type** - too many values are NA

- **Park_Borough** - this is the same as Borough

- **Park_Facility_Name** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **Community_Board** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **Descriptor** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **Closed_Date** - Variable is not needed because it is used to calculate ResolutionTime

- **Created_Date** - Variable is not needed because it is used to calculate ResolutionTime

```{r}
data2cand <- data2 %>% 
  select(-Unique_Key, -Agency_Name, -Incident_Address, -Cross_Street_1, -Cross_Street_2, -Intersection_Street_1, -Intersection_Street_2, -City, -Landmark, -Resolution_Description, -Street_Name, -X_Coordinate_State_Plane,  -Y_Coordinate_State_Plane, -Longitude, -Latitude, -Location, -Vehicle_Type, -Taxi_Company_Borough, -Taxi_Pick_Up_Location, -Bridge_Highway_Name, -Bridge_Highway_Direction, -Road_Ramp, -Bridge_Highway_Segment, -Due_Date, -Facility_Type, -Address_Type, -Park_Borough, -Park_Facility_Name, -Community_Board, -Descriptor, -Closed_Date, -Created_Date) %>% 
  mutate_if(is.character, factor)
data2cand <- na.omit(data2cand)

summary(data2cand)
nrow(data2cand)
```

#### Reduce Number of Categorical Variables to Less Than 53

Certain Categorical variables have greater than 53 categories. A Random Forest cannot be executed against these variables because of this. A solution would be to reduce the number of categories by combining like categories together.

```{r}
data2cand$Complaint_Type <- as.character(data2cand$Complaint_Type)

# Consolidate Noise variables
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Residential"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Helicopter"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Street/Sidewalk"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Commercial"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Vehicle"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - Park"] <- "Noise"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Noise - House of Worship"] <- "Noise"

# Consolidate Sanitary Condition variables
data2cand$Complaint_Type[data2cand$Complaint_Type == "UNSANITARY CONDITION"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Unsanitary Animal Pvt Property"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Indoor Sewage"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Unsanitary Pigeon Condition"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Dirty Conditions"] <- "Sanitation Condition"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Root/Sewer/Sidewalk Condition"] <- "Sanitation Condition"

# Consolidate Damaged Tree variables
data2cand$Complaint_Type[data2cand$Complaint_Type == "Damaged Tree"] <- "Tree Damage"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Overgrown Tree/Branches"] <- "Tree Damage"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Dead/Dying Tree"] <- "Tree Damage"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Illegal Tree Damage"] <- "Tree Damage"

# Consolidate Street Sign Issue variables
data2cand$Complaint_Type[data2cand$Complaint_Type == "Street Sign - Damaged"] <- "Street Sign Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Street Sign - Missing"] <- "Street Sign Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Street Sign - Dangling"] <- "Street Sign Issue"

# Consolidate Repair Issue variables
data2cand$Complaint_Type[data2cand$Complaint_Type == "PAINT/PLASTER"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "PLUMBING"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "ELECTRIC"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "APPLIANCE"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "HEAT/HOT WATER"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "WATER LEAK"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "FLOORING/STAIRS"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "DOOR/WINDOW"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "ELEVATOR"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "GENERAL"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "OUTSIDE BUILDING"] <- "Repair Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "SAFETY"] <- "Repair Issue"

# Consolidate Parking Violation variables
data2cand$Complaint_Type[data2cand$Complaint_Type == "Blocked Driveway"] <- "Parking Violation"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Illegal Parking"] <- "Parking Violation"

# Consolidate Homeless Issue variables
data2cand$Complaint_Type[data2cand$Complaint_Type == "Homeless Encampment"] <- "Homeless Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Homeless Person Assistance"] <- "Homeless Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Homeless Street Condition"] <- "Homeless Issue"

# Consolidate Animal Issue variables
data2cand$Complaint_Type[data2cand$Complaint_Type == "Animal-Abuse"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Animal in a Park"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Unleashed Dog"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Rodent"] <- "Animal Issue"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Mosquitoes"] <- "Animal Issue"

# Consolidate Dining Establishment variables
data2cand$Complaint_Type[data2cand$Complaint_Type == "Food Establishment"] <- "Dining Establishment"
data2cand$Complaint_Type[data2cand$Complaint_Type == "Outdoor Dining"] <- "Dining Establishment"

data2cand$Complaint_Type <- as.factor(data2cand$Complaint_Type)    
unique(data2cand$Complaint_Type)

data2cand$Location_Type <- as.character(data2cand$Location_Type)
data2cand$Location_Type[data2cand$Location_Type == "RESIDENTIAL BUILDING"] <- "Residential Property"
data2cand$Location_Type[data2cand$Location_Type == "Residential Building/House"] <- "Residential Property"
data2cand$Location_Type[data2cand$Location_Type == "Residential Building"] <- "Residential Property"
data2cand$Location_Type[data2cand$Location_Type == "Residential"] <- "Residential Property"
data2cand$Location_Type <- as.factor(data2cand$Location_Type)    
unique(data2cand$Location_Type)
```

#### Create Variable Acceptable_Resolution_Status

Upon executing the summary for ResolutionTime, we have a very wide set of values for ResolutionTime. The mean for ResolutionTime is an unacceptabe 4878 minutes while the median is an acceptable 32.4 minutes. We will create a variable called Acceptable_Resolution_Status with 'Y' for any value between 1 and 32.4 minutes and 'N' for 0 value or any value greater than 32.4 minutes.

```{r}
medianARS <- 32.4
data2cand$Acceptable_Resolution_Status = case_when(data2cand$ResolutionTime == 0 ~ 'N', 
                        (data2cand$ResolutionTime > 0 & data2cand$ResolutionTime <= medianARS) ~ 'Y', 
                        data2cand$ResolutionTime > medianARS ~ 'N')
summary(data2cand$ResolutionTime)
data2cand$Acceptable_Resolution_Status <- as.factor(data2cand$Acceptable_Resolution_Status)
summary(data2cand)
```

#### Split Data into Training and Test Data Sets

Next we will split the data into training and test data sets

```{r}
set.seed(100)
inTrain1 <- createDataPartition(y = data2cand$Acceptable_Resolution_Status, p=0.75, list = FALSE)
training1 <- data2cand[inTrain1,]
testing1 <- data2cand[-inTrain1,]
```

## (2) Random Forest {.tabset .tabset-fade .tabset-pills} 

The random forest algorithm works well with all sorts of data:
numeric and categorical, un-scaled and scaled, full rank and highly correlative.

#### Model 1 - Default (which has factor variables in single columns)

```{r}
set.seed(521)
m1 <- randomForest(Acceptable_Resolution_Status ~ ., data = training1)
print(m1)
plot(m1)
#Evaluate variable importance
importance(m1)
varImpPlot(m1)
```

We ran the random forest for the default model and we were very satisfied with the results. There is an error rate of 0%. We will find out later when we run the confusion matrix of the prediction values that Model 1 has an accuracy of 100%, a Kappa of 100%, and a Sensitivity of 100%. Moreover, a normal plot for a random forest should show a curve sloping down near the X and Y axes. There are 3 different curves almost parellel to each other. You will observe in the Variable Importance Plot and Summary that ResolutionTime has an extremely high Coeefficient. While it is a major predictor for the Application_Resolution_Status, it is highly correlated as we used the values of ResolutionTime to create the dependent variable Application_Resolution_Status. The next 3 most important variables afterwards - Resolution_Action_Updated_Date, Agency, and Complaint_Type - may show promise because they are variables from the original data set. We decided to keep the **m1** model as Model 1 for comparisons with the other Random Forest models in this section. 

#### Model 2 - Default Model minus ResolutionTime variable

```{r}
set.seed(521)
m2 <- randomForest(Acceptable_Resolution_Status ~ . - ResolutionTime, data = training1)
print(m2)
plot(m2)
#Evaluate variable importance
importance(m2)
varImpPlot(m2)
```

We ran the random forest for the default model and we were satisfied with the results. There is an error rate of 15.14%. We will find out later when we run the confusion matrix of the prediction values that Model 2 has an accuracy of 84.42%, a Kappa of 68.92%, and a Sensitivity of 77.05%. Moreover, a normal plot for a random forest should show a curve sloping down near the X and Y axes. There are 3 different curves almost parellel to each other. You will observe in the Variable Importance Plot and Summary that the 3 most important variables are Resolution_Action_Updated_Date, Agency, and Borough-Block-Lot (BBL). Complaint_Type is not too far behind. We decided to keep the **m2** model as Model 2 for comparisons with the other Random Forest models in this section. 

#### Model 3 - Default Model minus ResolutionTime variable with tuning

Prior to running Model 3, we ran tuneRF function to find out what is the optimal **mtry** variable that will have the lowest error rate and hopefully increase accuracy.

```{r}
# names of features
training1a <- training1 %>% 
  select(-ResolutionTime)
features <- setdiff(names(training1a), "Acceptable_Resolution_Status")

set.seed(123)

m3a <- tuneRF(
  x          = training1a[features],
  y          = training1a$Acceptable_Resolution_Status,
  ntreeTry   = 500,
  mtryStart  = 5,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)
best.m <- m3a[m3a[, 2] == min(m3a[, 2]), 1]
print(m3a)
print(best.m)
```

It was determined that the optiomal **mtry** parameter is 7. We reran the Random Forest to see if the accuracy improved.

```{r}
set.seed(521)
m3 <- randomForest(Acceptable_Resolution_Status ~ . - ResolutionTime, data = training1, mtry = 7)
print(m3)
plot(m3)
#Evaluate variable importance
importance(m3)
varImpPlot(m3)
```
We ran the random forest for the default model and we were satisfied with the results. There is an error rate of 13.68%. We will find out later when we run the confusion matrix of the prediction values that Model 2 has an accuracy of 85.65%, a Kappa of 71.32%, and a Sensitivity of 83%. Moreover, a normal plot for a random forest should show a curve sloping down near the X and Y axes. There are 3 different curves almost parellel to each other. You will observe in the Variable Importance Plot and Summary that the 3 most important variables are Resolution_Action_Updated_Date, Agency, and Borough-Block-Lot (BBL). We decided to keep the **m3** model as Model 3 for comparisons with the other Random Forest models in this section. 

## (3) Support Vector Machine {.tabset .tabset-fade .tabset-pills} 

TO-DO

## (4) Neural Network {.tabset .tabset-fade .tabset-pills} 

TO-DO

## (5) Model Performance and Selection {.tabset .tabset-fade .tabset-pills}

#### Random Forest

```{r}
prediction1 <- predict(m1,newdata = testing1)
prediction1.cm <- confusionMatrix(prediction1, testing1$Acceptable_Resolution_Status) 
prediction1.cm
```

```{r}
prediction2 <- predict(m2,newdata = testing1)
prediction2.cm <- confusionMatrix(prediction2, testing1$Acceptable_Resolution_Status) 
prediction2.cm
```

```{r}
prediction3 <- predict(m3,newdata = testing1)
prediction3.cm <- confusionMatrix(prediction3, testing1$Acceptable_Resolution_Status) 
prediction3.cm
```

```{r}
# Model 1 Values
prediction1.accuracy <- prediction1.cm$overall['Accuracy']
prediction1.kappa <- prediction1.cm$overall['Kappa']
prediction1.sensitivity <- prediction1.cm$byClass['Sensitivity']
prediction1.TN <- prediction1.cm$table[1,1]
prediction1.FP <- prediction1.cm$table[1,2]
prediction1.FN <- prediction1.cm$table[2,1]
prediction1.TP <- prediction1.cm$table[2,2]
prediction1.TPR <- prediction1.TP /(prediction1.TP + prediction1.FN)
prediction1.TNR <- prediction1.TN /(prediction1.TN + prediction1.FP)
prediction1.FPR <- prediction1.FP /(prediction1.TN + prediction1.FP)
prediction1.FNR <- prediction1.FN /(prediction1.TP + prediction1.FN)
prediction1.precision <- prediction1.TP / (prediction1.TP + prediction1.FP)
prediction1.recall <- prediction1.TP / (prediction1.TP + prediction1.FN)
prediction1.specificity <- prediction1.TN / (prediction1.TN + prediction1.FP)
prediction1.f1score <- 2 * ((prediction1.precision * prediction1.recall) / (prediction1.precision + prediction1.recall))
```

```{r}
# Model 2 Values
prediction2.accuracy <- prediction2.cm$overall['Accuracy']
prediction2.kappa <- prediction2.cm$overall['Kappa']
prediction2.sensitivity <- prediction2.cm$byClass['Sensitivity']
prediction2.TN <- prediction2.cm$table[1,1]
prediction2.FP <- prediction2.cm$table[1,2]
prediction2.FN <- prediction2.cm$table[2,1]
prediction2.TP <- prediction2.cm$table[2,2]
prediction2.TPR <- prediction2.TP /(prediction2.TP + prediction2.FN)
prediction2.TNR <- prediction2.TN /(prediction2.TN + prediction2.FP)
prediction2.FPR <- prediction2.FP /(prediction2.TN + prediction2.FP)
prediction2.FNR <- prediction2.FN /(prediction2.TP + prediction2.FN)
prediction2.precision <- prediction2.TP / (prediction2.TP + prediction2.FP)
prediction2.recall <- prediction2.TP / (prediction2.TP + prediction2.FN)
prediction2.specificity <- prediction2.TN / (prediction2.TN + prediction2.FP)
prediction2.f1score <- 2 * ((prediction2.precision * prediction2.recall) / (prediction2.precision + prediction2.recall))
```

```{r}
# Model 3 Values
prediction3.accuracy <- prediction3.cm$overall['Accuracy']
prediction3.kappa <- prediction3.cm$overall['Kappa']
prediction3.sensitivity <- prediction3.cm$byClass['Sensitivity']
prediction3.TN <- prediction3.cm$table[1,1]
prediction3.FP <- prediction3.cm$table[1,2]
prediction3.FN <- prediction3.cm$table[2,1]
prediction3.TP <- prediction3.cm$table[2,2]
prediction3.TPR <- prediction3.TP /(prediction3.TP + prediction3.FN)
prediction3.TNR <- prediction3.TN /(prediction3.TN + prediction3.FP)
prediction3.FPR <- prediction3.FP /(prediction3.TN + prediction3.FP)
prediction3.FNR <- prediction3.FN /(prediction3.TP + prediction3.FN)
prediction3.precision <- prediction3.TP / (prediction3.TP + prediction3.FP)
prediction3.recall <- prediction3.TP / (prediction3.TP + prediction3.FN)
prediction3.specificity <- prediction3.TN / (prediction3.TN + prediction3.FP)
prediction3.f1score <- 2 * ((prediction3.precision * prediction3.recall) / (prediction3.precision + prediction3.recall))
```

```{r, echo=FALSE}
Model <- c("Model 1","Model 2", "Model 3")
Accuracy <- c(prediction1.accuracy, prediction2.accuracy, prediction3.accuracy)
Kappa <- c(prediction1.kappa, prediction2.kappa, prediction3.kappa)
Sensitivity <- c(prediction1.sensitivity, prediction2.sensitivity, prediction3.sensitivity)
Recall <- c(prediction1.recall, prediction2.recall, prediction3.recall)
Specificity <- c(prediction1.specificity, prediction2.specificity, prediction3.specificity)
Precision <- c(prediction1.precision, prediction2.precision, prediction3.precision)
F1Score <- c(prediction1.f1score, prediction2.f1score, prediction3.f1score)
TPR <- c(prediction1.TPR, prediction2.TPR, prediction3.TPR)
TNR <- c(prediction1.TNR, prediction2.TNR, prediction3.TNR)
FPR <- c(prediction1.FPR, prediction2.FPR, prediction3.FPR)
FNR <- c(prediction1.FNR, prediction2.FNR, prediction3.FNR)
tableModel <- data.frame(Model,Accuracy,Kappa, Sensitivity, Recall,Specificity,Precision,F1Score)
tableModel %>%
  kable() %>%
  kable_styling()
```

Application process time
```{r}
# Stop the clock
proc.time() - ptm
```