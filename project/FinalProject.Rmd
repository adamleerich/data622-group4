---
title: "Group 4 Final Project"
author: "Ajay Arora, Romerl Elizes, Jimmy Ng, Joshua Registe, Adam Rich"
date: "May 23, 2021"
output:
  rmdformats::readthedown:
    self_contained: yes
    thumbnails: yes
    lightbox: yes
    gallery: no
    highlight: tango
    code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE,
  cache = FALSE)
packages <- c(
  'tidyverse', 
  'corrplot', 
  'palmerpenguins',
  'class',
  'kableExtra',
  'naniar',
  'mice',
  'DataExplorer',
  'caret',
  'tidymodels',
  'rsample',
  'themis',
  'randomForest',
  'car',
  'xgboost',
  'broom',
  'DBI',
  'RMySQL',
  'RJDBC'
)
for (pkg in packages) {
  suppressPackageStartupMessages(suppressWarnings(
    library(
      pkg, character.only = TRUE, 
      warn.conflicts = FALSE, quietly = TRUE)
  ))
}
# A ggplot2 function
defaulttheme <- theme(
  panel.background = element_blank(),
  panel.border = element_rect(color = "black", fill = NA))
```
 
## (Q1) Data Prep and Mitigation {.tabset .tabset-fade .tabset-pills} 

#### Introduction

After brainstorming the data sets available, Group 4 decided to use the New York City (NYC) 311 data found in the link: https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9. It is a very ambitious undertaking given that all Assignments covered in this course consisted of no more than 614 rows of records. The NYC data consists of 25+ million rows and 41 columns.
The initial goal of the project is to develop a prediction algorithm from 3 possible models that would allow 311 operators to give an estimate of when the request will be resolved?  It's a simple statement, but a complicated problem.  If the 311 department wanted to give people estimates they'd have to balance: 1) being conservative and overestimating, but people get angry cause it's going to take a long time to get done, 2) underestimating to make people feel optimistic, but then always failing, 3) giving a range and having that range be so big that an estimate is worthless, 4) giving an unbiased point estimate and always being wrong. It may be a huge undertaking, but Group 4 wanted to perform predictions based on not only a real data set but see if the algorithms provided by R can handle very large data sets.  

#### Data Download and Project Preparation

An initial problem with handling a data set with 25+ million rows is finding a repository within the time frame of the project that can meet the needs of the Group. There were two possible options: R-Studio Cloud Pro or Amazon Web Service. R-Studio Cloud Pro costs $25 per month at the student version but it could only handle up to 10 million rows of records. Romerl queried a sales representative from R-Studio Cloud Pro and got a quote of $300 plus a $50 per student cost for usage of such a large data set. which is unacceptable on a student budget for a one+ month project. Ajay found Amazon Web Service and it had four advantages: 1) it was free for one year, 2) it was able to house a table of 25+ million rows and more, 3) the data could be uploaded onto a mySQL database, 4) Group 4 team members could download the data on their own machines with the proper mySQL credentials provided by Ajay.

The next problem facing the Group is the initial downloading of the data. Ajay estimated a download time of 1.5 hours on R to mitigate the 25+ million rows. Ajay decided to split the massive table into 14 sub-tables of approximately 1.8 million rows each. The download time of data even at smaller queries were far more acceptable and allowed the Group to develop the Project further.

```{r}
# placeholder for maxrows
maxrows = 400
cn <- dbConnect(drv      = RMySQL::MySQL(),
                username = "data622",
                password = "group4622",
                host     = "data622.c3alv39zxnhk.us-east-2.rds.amazonaws.com",
                port     = 3306,
                dbname   = "")
# I queried column names from ny311 so we can decide which variables to eliminate and use a for-loop to concatenate a string for the select statement we will use to download data.
# - WE MAY NOT NEED THIS.
#colsqlstr <- "SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE table_name = 'data1';"
#colnms <- dbGetQuery(cn, colsqlstr)
#colnmslist <- list(colnms$COLUMN_NAME)
#str(colnmslist)
#mainsqlstr <- paste("SELECT * FROM DATA622.data1 order by rand() limit 0,",maxrows,";")
mainsqlstr <- paste("SELECT * FROM DATA622.data1 limit 0,",maxrows,";")
data2 <- dbGetQuery(cn, mainsqlstr)
str(data2)
summary(data2)
dbDisconnect(cn)
```

We performed some Data Type conversions in order to be able to execute preliminary Data Exploration activities.

```{r}
data2$Incident_Zip <- as.numeric(as.character(data2$Incident_Zip))
data2$BBL <- as.numeric(as.character(data2$BBL))
data2$X_Coordinate_State_Plane <- as.numeric(as.character(data2$X_Coordinate_State_Plane))
data2$Y_Coordinate_State_Plane <- as.numeric(as.character(data2$Y_Coordinate_State_Plane))
data2$Latitude <- as.numeric(as.character(data2$Latitude))
data2$Longitude <- as.numeric(as.character(data2$Longitude))
#data2[['Created_Date']] <- strptime(data2[['Created_Date']], format='%Y.%m.%d')
data2$Created_Date <- as.POSIXct(strptime(data2$Created_Date, "%Y-%m-%d %H:%M:%S"))
data2$Closed_Date <- as.POSIXct(strptime(data2$Closed_Date, "%Y-%m-%d %H:%M:%S"))
data2$Resolution_Action_Updated_Date <- as.POSIXct(strptime(data2$Resolution_Action_Updated_Date, "%Y-%m-%d %H:%M:%S"))
data2$Due_Date <- as.POSIXct(strptime(data2$Due_Date, "%Y-%m-%d %H:%M:%S"))

#data2[['Created_Date']] <- as.Date(data2[['Created_Date']])
#data2[['Closed_Date']] <- as.Date(data2[['Closed_Date']])
#data2[['Due_Date']] <- as.Date(data2[['Due_Date']])
#data2[['Resolution_Action_Updated_Date']] <- as.Date(data2[['Resolution_Action_Updated_Date']])
data2$ResolutionTime <- difftime(data2$Closed_Date,data2$Created_Date, units = "min")
```

#### Data Exploration

For our first Data Exploration activity, we did a plot of the Top Complaint Types

```{r}
#Top complaint types
ggplot(subset(data2, data2$Complaint_Type %in% count(data2, data2$Complaint_Type, sort=T)[1:maxrows,]$Complaint_Type), 
       aes(data2$Complaint_Type)) + 
  geom_histogram(stat = "count",color="black", fill="blue") +
  labs(x="Complaint Type", y="Service Requests") +
  coord_flip() + theme_bw()
```

For our second Data Exploration activity, we executed a plot for the Top Borough

```{r}
# Top Borough 
ggplot(subset(data2, data2$Borough %in% count(data2, data2$Borough, sort=T)[1:maxrows,]$Borough), 
       aes(data2$Borough)) + 
  geom_histogram(stat = "count",color="black", fill="blue") +
  labs(x="Borough", y="Service Requests") +
  coord_flip() + theme_bw()
```

For our third Data Exploration activity, we executed a plot of the Top Complaint Types by Borough and Status.

```{r}
# Top complaint types by Borough and Status 
ggplot(subset(data2, data2$Complaint_Type %in% count(data2, Complaint_Type, sort=T)[1:maxrows,]$Complaint_Type), aes(x=data2$Status, y = data2$Complaint_Type)) +
  geom_point() +
  geom_count(n=2, colour="black") + 
  facet_wrap(~data2$Borough)
```

#### Variable Elimination

We will have to get rid of some variables as they may not be needed for model building. For expediency, we will get rid of all NAs instead of imputing them.

- **Unique_Key** - continuous variable - ID - not needed.

- **Agency_Name** - full name of department or organization where 311 calls were made to. Agency is more appropriate.

- **Incident_Address** - full street address - too much disparate information to make an appropriate model

- **Street_Name** - Street name - too much disparate information to make an appropriate model

- **Cross_Street_1** - cross street - too much disparate information to make an appropriate model

- **Cross_Street_1** - cross street - too much disparate information to make an appropriate model

- **Intersection_Street_1** - intersection street - too much disparate information to make an appropriate model

- **Intersection_Street_2** - intersection street - too much disparate information to make an appropriate model

- **Landmark** - too much disparate information to make an appropriate model

- **City** - While NYC has 5 boroughs, Queens has multiple cities which will provide too much disparate information to make an appropriate model.

- **Resolution_Description** - detailed description of resolution - too much disparate information to make an appropriate model

- **X_Coordinate_State_Plane** - x coordinate of approximate location - too much disparate information to make an appropriate model 

- **Y_Coordinate_State_Plane** - y coordinate of approximate location - too much disparate information to make an appropriate model

- **Longitude** - longitude of approximate location - too much disparate information to make an appropriate model

- **Latitude** - latitude of approximate location - too much disparate information to make an appropriate model

- **Location**  - set of latitude and longitude of approximate location - too much disparate information to make an appropriate model

- **Vehicle_Type** - almost all values are NA 

- **Taxi_Company_Borough** - almost all values are NA 

- **Taxi_Pick_Up_Location** - almost all values are NA 

- **Bridge_Highway_Name** - almost all values are NA 

- **Bridge_Highway_Direction** - almost all values are NA 

- **Road_Ramp** - almost all values are NA 

- **Bridge_Highway_Segment** - almost all values are NA

- **Bridge_Highway_Segment** - almost all values are NA

- **Due_Date** - almost all values are NA

- **Facility_Type** - too many values are NA

- **Address_Type** - too many values are NA

- **Park_Borough** - this is the same as Borough

- **Park_Facility_Name** - Random Forest algorithm cannot handle this variable as it has over 53 categories

- **Community_Board** - Random Forest algorithm cannot handle this variable as it has over 53 categories

```{r}
data2cand <- data2 %>% 
  select(-Unique_Key, -Agency_Name, -Incident_Address, -Cross_Street_1, -Cross_Street_2, -Intersection_Street_1, -Intersection_Street_2, -Landmark, -City, -Street_Name, -Resolution_Description, -X_Coordinate_State_Plane,  -Y_Coordinate_State_Plane, -Longitude, -Latitude, -Location, -Vehicle_Type, -Taxi_Company_Borough, -Taxi_Pick_Up_Location, -Bridge_Highway_Name, -Bridge_Highway_Direction, -Road_Ramp, -Bridge_Highway_Segment, -Due_Date, -Facility_Type, -Address_Type, -Park_Borough, -Park_Facility_Name, -Community_Board) %>% 
  mutate_if(is.character, factor)
data2cand <- na.omit(data2cand)

summary(data2cand)
nrow(data2cand)
#head(data2cand,100)
```

#### Split Data into Training and Test Data Sets

Next we will split the data into training and test data sets

```{r}
set.seed(100)
inTrain1 <- createDataPartition(y = data2cand$ResolutionTime, p=0.75, list = FALSE)
training1 <- data2cand[inTrain1,]
testing1 <- data2cand[-inTrain1,]
```

## (Q2) Random Forest {.tabset .tabset-fade .tabset-pills} 

TO-DO

```{r}
#set.seed(521)
#m1 <- randomForest(ResolutionTime ~ ., data = training1)
#print(m1)
```

## (Q3) Support Vector Machine {.tabset .tabset-fade .tabset-pills} 

TO-DO

## (Q4) Neural Network {.tabset .tabset-fade .tabset-pills} 

TO-DO

## (Q5) Model Performance and Selection {.tabset .tabset-fade .tabset-pills}

TO-DO